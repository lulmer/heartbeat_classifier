{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "edts_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lulmer/heartbeat_classifier/blob/master/edts_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O7wI5a06nzy",
        "colab_type": "code",
        "outputId": "189d20f6-cb52-4547-d9a9-6efc3d0b6735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Montage du disque Google Drive sur le point /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzBEc6nz7ArN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chemin d'accès au dataset\n",
        "path = \"drive/My Drive/INSA/ASI 5/EDTS/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQBfDLc96lod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports des utilitaires\n",
        "import scipy.io.wavfile\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm._tqdm_notebook import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNALn7ZS6loh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = list(Path(path + \"dataset\").rglob(\"Btraining_*/*.wav\")) # Liste contenant le chemin d'accès aux fichiers du dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKD1KU4g6lo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction retournant le label d'un fichier\n",
        "def labelling(cats, file):\n",
        "    for i, cat in enumerate(cats):\n",
        "        if cat in file.parts[6]:\n",
        "            return i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Xmuf7Z6lou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats = np.unique([file.parts[6] for file in files]) # Noms des différentes classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CN_FnOTcRFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats_files = [labelling(cats, file) for file in files] # Liste contenant tous les labels des fichiers du dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7q9agmM6lol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Séparation en jeux d'apprentissage, validation et test\n",
        "# Utilisation de stratify = mêmes proportions de classes dans chacun des jeux créés\n",
        "idx_train, idx_val = train_test_split(range(len(files)), test_size=0.3, stratify=cats_files)\n",
        "idx_train, idx_test = train_test_split(idx_train, test_size=33, stratify=np.array(cats_files)[idx_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALZS_HGc8Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Récupération des chemins vers les fichiers suivant la répartition en jeux d'apprentissage, validation et test\n",
        "files_train = np.array(files)[idx_train]\n",
        "files_val = np.array(files)[idx_val]\n",
        "files_test = np.array(files)[idx_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNyyNs938ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Récupération des labels des jeux d'apprentissage, validation et test\n",
        "y_train = np.array(cats_files)[idx_train]\n",
        "y_val = np.array(cats_files)[idx_val]\n",
        "y_test = np.array(cats_files)[idx_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj_uxEEp6lpJ",
        "colab_type": "code",
        "outputId": "4a53baac-ef38-4c7c-cdab-dd2417a236e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        }
      },
      "source": [
        "# Imports des modules de Deep Learning\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Dropout, Softmax, Conv2D, MaxPool2D, Flatten \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n452TQCyZ86i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction convertissant une liste de fichiers en liste de spectrogrammes de Mel\n",
        "def to_image(files):\n",
        "  x = np.empty((len(files), 128, 98)) # Initialisation de la matrice\n",
        "  for i, file in enumerate(tqdm(files)): # Pour chaque fichier de la liste (tqdm => affichage d'une barre de progression)\n",
        "    y, sr = librosa.load(file) # Ouverture du fichier, y = signal, sr = fréquence d'échantillonnage\n",
        "    # Pour que les spectrogrammes fassent tous la même taille, on fixe la longueur des signaux\n",
        "    if len(y) > 50000: # Si le signal est plus grand que 50000 valeurs\n",
        "      y = y[:50000] # On ne prend que les 50000 premières\n",
        "    elif len(y) < 50000: # Sinon\n",
        "      n_repet = int(np.ceil(50000/len(y))) # On calcule le nombre de répétitions du signal nécessaires\n",
        "      y = np.tile(y, n_repet)[:50000] # On répète le signal suivant ce nombre, et on en garde les 50000 premières valeurs\n",
        "    S = librosa.feature.melspectrogram(y, sr=sr, n_fft=2048, hop_length=512) # Calcul du spectrogramme de Mel (taille de fenêtre FFT = 2048, temps entre 2 frames = 512)(cf. papiers)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max) # Conversion en décibels\n",
        "    ss = (S_DB - np.min(S_DB))/(np.max(S_DB)-np.min(S_DB)) # Normalisation min-max du spectrogramme => valeurs dans [0..1]\n",
        "    x[i,:,:] = ss # On ajoute le spectrogramme à la matrice\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41HOvbGjbHKC",
        "colab_type": "code",
        "outputId": "3d3ce351-8467-471d-ccd6-fb4ed116225f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Création des matrices d'apprentissage, validation et test\n",
        "x_train = to_image(files_train)\n",
        "x_val = to_image(files_val)\n",
        "x_test = to_image(files_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 289/289 [02:53<00:00,  1.62it/s]\n",
            "100%|██████████| 139/139 [01:23<00:00,  1.63it/s]\n",
            "100%|██████████| 33/33 [00:20<00:00,  1.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7JMSG-sgk5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN attendent une entrée de taille (TAILLE_BATCH, HAUTEUR, LARGEUR, CANAUX) => on ajoute une dimension de 1 pour nos canaux\n",
        "x_train = np.expand_dims(x_train, 3)\n",
        "x_val = np.expand_dims(x_val, 3)\n",
        "x_test = np.expand_dims(x_test, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsbKTIeJstSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modèle de CNN\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(128, 98, 1)))\n",
        "cnn.add(Conv2D(32, 3))\n",
        "cnn.add(MaxPool2D(2))\n",
        "cnn.add(Conv2D(64, 3))\n",
        "cnn.add(MaxPool2D(2))\n",
        "cnn.add(Conv2D(128, 3))\n",
        "cnn.add(MaxPool2D(2))\n",
        "cnn.add(Conv2D(256, 3))\n",
        "cnn.add(MaxPool2D(2))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(3))\n",
        "cnn.add(Softmax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ath0uw2Usgnx",
        "colab_type": "code",
        "outputId": "33d8d31c-f44e-4ab7-f1eb-8364c4ba84f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "# Affichage des couches, tailles et paramètres du modèle\n",
        "cnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_88\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_309 (Conv2D)          (None, 126, 96, 32)       320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_308 (MaxPoolin (None, 63, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_310 (Conv2D)          (None, 61, 46, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_309 (MaxPoolin (None, 30, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_311 (Conv2D)          (None, 28, 21, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_310 (MaxPoolin (None, 14, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_312 (Conv2D)          (None, 12, 8, 256)        295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_311 (MaxPoolin (None, 6, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_87 (Flatten)         (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 3)                 18435     \n",
            "_________________________________________________________________\n",
            "softmax_87 (Softmax)         (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 406,275\n",
            "Trainable params: 406,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJEQG4M6lpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilisation de l'optimiseur Adam, car propose un taux d'apprentissage adaptatif\n",
        "opti = Adam(lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9LtbnX6lpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compilation du graphe du modèle\n",
        "# Classification => Crossentropy\n",
        "# On cherche à maximiser la précision du modèle, en minimisant son entropie croisée\n",
        "cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=opti)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAernj9mIlrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbacks Keras\n",
        "# Sauvegarde les poids du modèle à l'epoch où la précision est maximal\n",
        "chkpt_acc = ModelCheckpoint(filepath=path+\"best_acc.hdf5\", monitor=\"val_acc\", save_best_only=True, mode=\"max\", verbose=1)\n",
        "# Sauvegarde les poids du modèle à l'epoch où la fonction de coût est minimale\n",
        "chkpt_loss = ModelCheckpoint(filepath=path+\"best_loss.hdf5\", monitor=\"val_loss\", save_best_only=True, mode=\"min\", verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlOx9EZ56lpb",
        "colab_type": "code",
        "outputId": "aaeb0d8d-d68c-46a1-d5c1-4a5dd7d486cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Apprentissage\n",
        "# Hyperparamètres choisis suite à plusieurs essais\n",
        "history = cnn.fit(x=x_train, y=to_categorical(y_train), epochs=40, batch_size=17, validation_data=(x_val, to_categorical(y_val)), callbacks=[chkpt_acc, chkpt_loss])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "288/289 [============================>.] - ETA: 0s - loss: 180.0770 - acc: 0.4757\n",
            "Epoch 00001: val_acc improved from -inf to 0.69065, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 61.20735, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 28s 98ms/sample - loss: 179.4727 - acc: 0.4740 - val_loss: 61.2073 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 74.2897 - acc: 0.5000\n",
            "Epoch 00002: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00002: val_loss improved from 61.20735 to 15.82359, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 66.1196 - acc: 0.5121 - val_loss: 15.8236 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 22.7012 - acc: 0.5500\n",
            "Epoch 00003: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00003: val_loss improved from 15.82359 to 11.50334, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 21.0552 - acc: 0.5363 - val_loss: 11.5033 - val_acc: 0.1007\n",
            "Epoch 4/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 4.9087 - acc: 0.5873\n",
            "Epoch 00004: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00004: val_loss improved from 11.50334 to 1.05548, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 4.4599 - acc: 0.5952 - val_loss: 1.0555 - val_acc: 0.4029\n",
            "Epoch 5/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.9224 - acc: 0.6667\n",
            "Epoch 00005: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.05548 to 0.86435, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.9072 - acc: 0.6644 - val_loss: 0.8644 - val_acc: 0.6906\n",
            "Epoch 6/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.8603 - acc: 0.7103\n",
            "Epoch 00006: val_acc improved from 0.69065 to 0.72662, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.86435 to 0.82177, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.8573 - acc: 0.6990 - val_loss: 0.8218 - val_acc: 0.7266\n",
            "Epoch 7/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.8598 - acc: 0.6667\n",
            "Epoch 00007: val_acc did not improve from 0.72662\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.82177 to 0.75486, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.8482 - acc: 0.6678 - val_loss: 0.7549 - val_acc: 0.6906\n",
            "Epoch 8/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.8091 - acc: 0.6944\n",
            "Epoch 00008: val_acc did not improve from 0.72662\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.75486\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7905 - acc: 0.7128 - val_loss: 0.7625 - val_acc: 0.6978\n",
            "Epoch 9/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.8039 - acc: 0.6875\n",
            "Epoch 00009: val_acc did not improve from 0.72662\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.75486\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7951 - acc: 0.6955 - val_loss: 0.8206 - val_acc: 0.6906\n",
            "Epoch 10/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7882 - acc: 0.6905\n",
            "Epoch 00010: val_acc did not improve from 0.72662\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.75486 to 0.72921, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.7765 - acc: 0.7024 - val_loss: 0.7292 - val_acc: 0.7050\n",
            "Epoch 11/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7654 - acc: 0.7222\n",
            "Epoch 00011: val_acc improved from 0.72662 to 0.74101, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72921 to 0.72328, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.7705 - acc: 0.7163 - val_loss: 0.7233 - val_acc: 0.7410\n",
            "Epoch 12/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7082 - acc: 0.7460\n",
            "Epoch 00012: val_acc improved from 0.74101 to 0.74820, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.72328\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.7356 - acc: 0.7301 - val_loss: 0.7625 - val_acc: 0.7482\n",
            "Epoch 13/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.7536 - acc: 0.7250\n",
            "Epoch 00013: val_acc did not improve from 0.74820\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.72328\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7663 - acc: 0.7128 - val_loss: 0.7295 - val_acc: 0.7338\n",
            "Epoch 14/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7091 - acc: 0.7302\n",
            "Epoch 00014: val_acc did not improve from 0.74820\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72328 to 0.69786, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.7326 - acc: 0.7266 - val_loss: 0.6979 - val_acc: 0.7482\n",
            "Epoch 15/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.7021 - acc: 0.7292\n",
            "Epoch 00015: val_acc did not improve from 0.74820\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69786\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7098 - acc: 0.7266 - val_loss: 0.7108 - val_acc: 0.7122\n",
            "Epoch 16/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6928 - acc: 0.7500\n",
            "Epoch 00016: val_acc improved from 0.74820 to 0.75540, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.69786 to 0.67797, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.6917 - acc: 0.7509 - val_loss: 0.6780 - val_acc: 0.7554\n",
            "Epoch 17/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6562 - acc: 0.7579\n",
            "Epoch 00017: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6731 - acc: 0.7405 - val_loss: 0.8115 - val_acc: 0.6547\n",
            "Epoch 18/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7756 - acc: 0.6667\n",
            "Epoch 00018: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7662 - acc: 0.6747 - val_loss: 0.7227 - val_acc: 0.7266\n",
            "Epoch 19/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.8079 - acc: 0.7103\n",
            "Epoch 00019: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.8094 - acc: 0.6920 - val_loss: 0.7791 - val_acc: 0.6763\n",
            "Epoch 20/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7212 - acc: 0.7262\n",
            "Epoch 00020: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7135 - acc: 0.7301 - val_loss: 0.6838 - val_acc: 0.7482\n",
            "Epoch 21/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.6732 - acc: 0.7625\n",
            "Epoch 00021: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6541 - acc: 0.7751 - val_loss: 0.6981 - val_acc: 0.7482\n",
            "Epoch 22/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6337 - acc: 0.7778\n",
            "Epoch 00022: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67797\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6477 - acc: 0.7647 - val_loss: 0.8733 - val_acc: 0.6619\n",
            "Epoch 23/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.7602 - acc: 0.7262\n",
            "Epoch 00023: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.67797 to 0.67162, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.7438 - acc: 0.7370 - val_loss: 0.6716 - val_acc: 0.7554\n",
            "Epoch 24/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6298 - acc: 0.7500\n",
            "Epoch 00024: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67162\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6485 - acc: 0.7474 - val_loss: 0.8040 - val_acc: 0.6475\n",
            "Epoch 25/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6969 - acc: 0.7103\n",
            "Epoch 00025: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67162\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6562 - acc: 0.7370 - val_loss: 0.8487 - val_acc: 0.7554\n",
            "Epoch 26/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.6549 - acc: 0.7583\n",
            "Epoch 00026: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67162\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6262 - acc: 0.7751 - val_loss: 0.6841 - val_acc: 0.7482\n",
            "Epoch 27/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6238 - acc: 0.7540\n",
            "Epoch 00027: val_acc improved from 0.75540 to 0.76978, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.67162 to 0.64615, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.6120 - acc: 0.7578 - val_loss: 0.6461 - val_acc: 0.7698\n",
            "Epoch 28/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.6271 - acc: 0.7540\n",
            "Epoch 00028: val_acc did not improve from 0.76978\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.64615\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6307 - acc: 0.7509 - val_loss: 0.7402 - val_acc: 0.7482\n",
            "Epoch 29/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.5914 - acc: 0.7659\n",
            "Epoch 00029: val_acc did not improve from 0.76978\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.64615 to 0.63312, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.5833 - acc: 0.7716 - val_loss: 0.6331 - val_acc: 0.7626\n",
            "Epoch 30/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.5584 - acc: 0.7937\n",
            "Epoch 00030: val_acc did not improve from 0.76978\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.63312 to 0.62423, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.5676 - acc: 0.7889 - val_loss: 0.6242 - val_acc: 0.7554\n",
            "Epoch 31/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.5533 - acc: 0.7708\n",
            "Epoch 00031: val_acc improved from 0.76978 to 0.79856, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.62423\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.5388 - acc: 0.7820 - val_loss: 0.6284 - val_acc: 0.7986\n",
            "Epoch 32/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4674 - acc: 0.8135\n",
            "Epoch 00032: val_acc improved from 0.79856 to 0.81295, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.62423 to 0.61787, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.5062 - acc: 0.7924 - val_loss: 0.6179 - val_acc: 0.8129\n",
            "Epoch 33/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.5143 - acc: 0.8135\n",
            "Epoch 00033: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.61787 to 0.60705, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.5018 - acc: 0.8201 - val_loss: 0.6071 - val_acc: 0.7770\n",
            "Epoch 34/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4913 - acc: 0.8016\n",
            "Epoch 00034: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4801 - acc: 0.8097 - val_loss: 0.6176 - val_acc: 0.7770\n",
            "Epoch 35/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.5116 - acc: 0.7857\n",
            "Epoch 00035: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4968 - acc: 0.7993 - val_loss: 0.6827 - val_acc: 0.7626\n",
            "Epoch 36/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4924 - acc: 0.8214\n",
            "Epoch 00036: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4952 - acc: 0.8131 - val_loss: 0.6389 - val_acc: 0.7698\n",
            "Epoch 37/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4688 - acc: 0.8135\n",
            "Epoch 00037: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4631 - acc: 0.8166 - val_loss: 0.6090 - val_acc: 0.7914\n",
            "Epoch 38/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4455 - acc: 0.8254\n",
            "Epoch 00038: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4602 - acc: 0.8201 - val_loss: 0.6210 - val_acc: 0.7626\n",
            "Epoch 39/40\n",
            "252/289 [=========================>....] - ETA: 0s - loss: 0.4538 - acc: 0.8294\n",
            "Epoch 00039: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.4699 - acc: 0.8201 - val_loss: 0.8621 - val_acc: 0.5468\n",
            "Epoch 40/40\n",
            "240/289 [=======================>......] - ETA: 0s - loss: 0.9948 - acc: 0.6917\n",
            "Epoch 00040: val_acc did not improve from 0.81295\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.60705\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.8906 - acc: 0.7266 - val_loss: 1.3676 - val_acc: 0.7554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvVbRfa26lpf",
        "colab_type": "code",
        "outputId": "f9dc3a01-bc34-49c6-fbae-af287dae9767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "source": [
        "# Affichage des courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.legend([\"Training\", \"Validation\"])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.figure()\n",
        "plt.plot(np.log(history.history['loss']))\n",
        "plt.plot(np.log(history.history['val_loss']))\n",
        "plt.legend([\"Training\", \"Validation\"])\n",
        "plt.title(\"LogLoss\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'LogLoss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 624
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14aIbSvnBZnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du modèle de fonction de coût minime\n",
        "mdl = load_model(path + \"best_loss.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3lOp4OIBjbw",
        "colab_type": "code",
        "outputId": "7ab21603-7191-4d0b-e37d-a3cbf28c09f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Calcul du score de précision de ce modèle sur le jeu de test\n",
        "mdl.evaluate(x_test, to_categorical(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 2s 62ms/sample - loss: 0.7193 - acc: 0.7879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7192944541121974, 0.7878788]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 622
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moc6tlRTFWxG",
        "colab_type": "text"
      },
      "source": [
        "# KFold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4A1AfRZBKBS",
        "colab": {}
      },
      "source": [
        "# StratifiedKFold => chaque sous ensemble possède la même répartition de classes que les autres => représentativité\n",
        "fold = StratifiedKFold(n_splits=5, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ed3ee68b-f12f-4ed6-ebd7-da3662ad424a",
        "id": "P8SwbYE_BJic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Calcul des spectrogrammes de tous les fichiers du dataset\n",
        "cv_all = to_image(files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461/461 [04:11<00:00,  1.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljURAHw2BJC6",
        "colab": {}
      },
      "source": [
        "# Ajout d'une dimension pour les canaux\n",
        "cv_all = np.expand_dims(cv_all, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9XT0az76TYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modèle CNN\n",
        "def CNN():\n",
        "  cnn = Sequential()\n",
        "  cnn.add(InputLayer(input_shape=(128, 98, 1)))\n",
        "  cnn.add(Conv2D(32, 3))\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(64, 3))\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(128, 3))\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(256, 3))\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Flatten())\n",
        "  cnn.add(Dense(3))\n",
        "  cnn.add(Softmax())\n",
        "  return cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nce6D-29e1K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback\n",
        "# Stoppe l'apprentissage quand la fonction de coût n'évolue plus au bout de \"patience\" epochs\n",
        "# À la fin de l'apprentissage, les poids de l'epoch de coût minimal sons restaurés\n",
        "# => Permet de ne pas avoir à sauvegarder le meilleur modèle\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=40, mode=\"min\", restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbWZBk2_5g1t",
        "colab_type": "code",
        "outputId": "408d14ae-841f-4d53-fc7e-c31e09b4d366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Boucle de cross-validation\n",
        "cv = [] # Liste contenant les scores de précision de chaque itération\n",
        "for cv_train_idx, cv_val_idx in fold.split(cv_all, cats_files): # Récupération des indices de jeux d'apprentissage, validation et test de l'itération en cours\n",
        "  x_cv_val = cv_all[cv_val_idx] # Jeu de validation\n",
        "  y_cv_val = np.array(cats_files)[cv_val_idx] # Labels de validation\n",
        "  cv_tt_idx, cv_tt_idx = train_test_split(cv_train_idx, test_size=0.2, stratify=np.array(cats_files)[cv_tt_idx]) # Indices des jeux d'apprentissage et test\n",
        "  cv_x_train, cv_x_test = cv_all[cv_tt_idx], cv_all[cv_tt_idx] # Jeux d'apprentissage et test\n",
        "  cv_y_train, cv_y_test = np.array(cats_files)[cv_tt_idx], np.array(cats_files)[cv_tt_idx] # Labels d'apprentissage et test\n",
        "  model = CNN() # Création d'un nouveau modèle\n",
        "  model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=\"adam\") # Compilation\n",
        "  model.fit(x_train, to_categorical(y_train), epochs=40, batch_size=17, validation_data=(x_val, to_categorical(y_val)), callbacks=[es]) # Apprentissage\n",
        "  cv.append(model.evaluate(x_test, to_categorical(y_test))[1]) # On ajoute la précision de l'itération à la liste"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 294 samples, validate on 93 samples\n",
            "Epoch 1/40\n",
            "294/294 [==============================] - 9s 29ms/sample - loss: 0.9684 - acc: 0.6054 - val_loss: 0.9034 - val_acc: 0.6882\n",
            "Epoch 2/40\n",
            "294/294 [==============================] - 0s 1ms/sample - loss: 0.8354 - acc: 0.6769 - val_loss: 0.8506 - val_acc: 0.6882\n",
            "Epoch 3/40\n",
            "294/294 [==============================] - 0s 968us/sample - loss: 0.7993 - acc: 0.6939 - val_loss: 0.8662 - val_acc: 0.6882\n",
            "Epoch 4/40\n",
            "294/294 [==============================] - 0s 909us/sample - loss: 0.8158 - acc: 0.6939 - val_loss: 0.8961 - val_acc: 0.6882\n",
            "Epoch 5/40\n",
            "294/294 [==============================] - 0s 899us/sample - loss: 0.7876 - acc: 0.6939 - val_loss: 0.8003 - val_acc: 0.6882\n",
            "Epoch 6/40\n",
            "294/294 [==============================] - 0s 865us/sample - loss: 0.7449 - acc: 0.7245 - val_loss: 0.8639 - val_acc: 0.6882\n",
            "Epoch 7/40\n",
            "294/294 [==============================] - 0s 856us/sample - loss: 0.7928 - acc: 0.7007 - val_loss: 0.7787 - val_acc: 0.7204\n",
            "Epoch 8/40\n",
            "294/294 [==============================] - 0s 863us/sample - loss: 0.6868 - acc: 0.7415 - val_loss: 0.7417 - val_acc: 0.7742\n",
            "Epoch 9/40\n",
            "294/294 [==============================] - 0s 871us/sample - loss: 0.6061 - acc: 0.7789 - val_loss: 0.6308 - val_acc: 0.7849\n",
            "Epoch 10/40\n",
            "294/294 [==============================] - 0s 872us/sample - loss: 0.5417 - acc: 0.8027 - val_loss: 0.6186 - val_acc: 0.7742\n",
            "Epoch 11/40\n",
            "294/294 [==============================] - 0s 842us/sample - loss: 0.4670 - acc: 0.8231 - val_loss: 0.6826 - val_acc: 0.7634\n",
            "Epoch 12/40\n",
            "294/294 [==============================] - 0s 824us/sample - loss: 0.3938 - acc: 0.8503 - val_loss: 0.6483 - val_acc: 0.7957\n",
            "Epoch 13/40\n",
            "294/294 [==============================] - 0s 826us/sample - loss: 0.3143 - acc: 0.8741 - val_loss: 1.2897 - val_acc: 0.7527\n",
            "Epoch 14/40\n",
            "294/294 [==============================] - 0s 834us/sample - loss: 0.4431 - acc: 0.8537 - val_loss: 0.8308 - val_acc: 0.7312\n",
            "Epoch 15/40\n",
            "294/294 [==============================] - 0s 831us/sample - loss: 0.2415 - acc: 0.9116 - val_loss: 1.0037 - val_acc: 0.7312\n",
            "Epoch 16/40\n",
            "294/294 [==============================] - 0s 817us/sample - loss: 0.1310 - acc: 0.9592 - val_loss: 1.1352 - val_acc: 0.6989\n",
            "Epoch 17/40\n",
            "294/294 [==============================] - 0s 824us/sample - loss: 0.0785 - acc: 0.9762 - val_loss: 1.3535 - val_acc: 0.7204\n",
            "Epoch 18/40\n",
            "294/294 [==============================] - 0s 846us/sample - loss: 0.0425 - acc: 0.9932 - val_loss: 1.5772 - val_acc: 0.6989\n",
            "Epoch 19/40\n",
            "294/294 [==============================] - 0s 843us/sample - loss: 0.0679 - acc: 0.9728 - val_loss: 1.4848 - val_acc: 0.6452\n",
            "Epoch 20/40\n",
            "294/294 [==============================] - 0s 818us/sample - loss: 0.0397 - acc: 0.9932 - val_loss: 1.9635 - val_acc: 0.7097\n",
            "Epoch 21/40\n",
            "294/294 [==============================] - 0s 834us/sample - loss: 0.0256 - acc: 0.9932 - val_loss: 1.7896 - val_acc: 0.7097\n",
            "Epoch 22/40\n",
            "294/294 [==============================] - 0s 849us/sample - loss: 0.0097 - acc: 1.0000 - val_loss: 1.8763 - val_acc: 0.7204\n",
            "Epoch 23/40\n",
            "294/294 [==============================] - 0s 815us/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 1.9465 - val_acc: 0.7204\n",
            "Epoch 24/40\n",
            "294/294 [==============================] - 0s 825us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 2.0200 - val_acc: 0.7204\n",
            "Epoch 25/40\n",
            "294/294 [==============================] - 0s 832us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 2.0468 - val_acc: 0.7097\n",
            "Epoch 26/40\n",
            "294/294 [==============================] - 0s 831us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 2.1263 - val_acc: 0.7097\n",
            "Epoch 27/40\n",
            "294/294 [==============================] - 0s 839us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.1841 - val_acc: 0.7097\n",
            "Epoch 28/40\n",
            "294/294 [==============================] - 0s 825us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 2.1939 - val_acc: 0.7097\n",
            "Epoch 29/40\n",
            "294/294 [==============================] - 0s 835us/sample - loss: 9.1285e-04 - acc: 1.0000 - val_loss: 2.2299 - val_acc: 0.7097\n",
            "Epoch 30/40\n",
            "294/294 [==============================] - 0s 823us/sample - loss: 8.1299e-04 - acc: 1.0000 - val_loss: 2.2923 - val_acc: 0.7097\n",
            "Epoch 31/40\n",
            "294/294 [==============================] - 0s 870us/sample - loss: 7.2401e-04 - acc: 1.0000 - val_loss: 2.2951 - val_acc: 0.7097\n",
            "Epoch 32/40\n",
            "294/294 [==============================] - 0s 828us/sample - loss: 6.5780e-04 - acc: 1.0000 - val_loss: 2.3158 - val_acc: 0.7204\n",
            "Epoch 33/40\n",
            "294/294 [==============================] - 0s 849us/sample - loss: 5.8798e-04 - acc: 1.0000 - val_loss: 2.3525 - val_acc: 0.7204\n",
            "Epoch 34/40\n",
            "294/294 [==============================] - 0s 811us/sample - loss: 5.4197e-04 - acc: 1.0000 - val_loss: 2.3788 - val_acc: 0.7204\n",
            "Epoch 35/40\n",
            "294/294 [==============================] - 0s 857us/sample - loss: 4.9909e-04 - acc: 1.0000 - val_loss: 2.4126 - val_acc: 0.7097\n",
            "Epoch 36/40\n",
            "294/294 [==============================] - 0s 833us/sample - loss: 4.6250e-04 - acc: 1.0000 - val_loss: 2.4314 - val_acc: 0.7312\n",
            "Epoch 37/40\n",
            "294/294 [==============================] - 0s 802us/sample - loss: 4.2736e-04 - acc: 1.0000 - val_loss: 2.4422 - val_acc: 0.7204\n",
            "Epoch 38/40\n",
            "294/294 [==============================] - 0s 813us/sample - loss: 3.9896e-04 - acc: 1.0000 - val_loss: 2.4693 - val_acc: 0.7204\n",
            "Epoch 39/40\n",
            "294/294 [==============================] - 0s 872us/sample - loss: 3.7193e-04 - acc: 1.0000 - val_loss: 2.4828 - val_acc: 0.7312\n",
            "Epoch 40/40\n",
            "294/294 [==============================] - 0s 838us/sample - loss: 3.5052e-04 - acc: 1.0000 - val_loss: 2.5130 - val_acc: 0.7204\n",
            "74/74 [==============================] - 0s 379us/sample - loss: 2.7768 - acc: 0.7027\n",
            "Train on 295 samples, validate on 92 samples\n",
            "Epoch 1/40\n",
            "295/295 [==============================] - 9s 29ms/sample - loss: 0.8874 - acc: 0.6881 - val_loss: 0.8213 - val_acc: 0.6957\n",
            "Epoch 2/40\n",
            "295/295 [==============================] - 0s 1ms/sample - loss: 0.8353 - acc: 0.6949 - val_loss: 0.8025 - val_acc: 0.6957\n",
            "Epoch 3/40\n",
            "295/295 [==============================] - 0s 950us/sample - loss: 0.8032 - acc: 0.6949 - val_loss: 0.8953 - val_acc: 0.6957\n",
            "Epoch 4/40\n",
            "295/295 [==============================] - 0s 915us/sample - loss: 0.8302 - acc: 0.6983 - val_loss: 0.7913 - val_acc: 0.6957\n",
            "Epoch 5/40\n",
            "295/295 [==============================] - 0s 887us/sample - loss: 0.7416 - acc: 0.7119 - val_loss: 0.7743 - val_acc: 0.7174\n",
            "Epoch 6/40\n",
            "295/295 [==============================] - 0s 897us/sample - loss: 0.6831 - acc: 0.7593 - val_loss: 0.7613 - val_acc: 0.7283\n",
            "Epoch 7/40\n",
            "295/295 [==============================] - 0s 890us/sample - loss: 0.6021 - acc: 0.7729 - val_loss: 0.7110 - val_acc: 0.7283\n",
            "Epoch 8/40\n",
            "295/295 [==============================] - 0s 841us/sample - loss: 0.5512 - acc: 0.7898 - val_loss: 0.7521 - val_acc: 0.7500\n",
            "Epoch 9/40\n",
            "295/295 [==============================] - 0s 839us/sample - loss: 0.4912 - acc: 0.8136 - val_loss: 0.7942 - val_acc: 0.7391\n",
            "Epoch 10/40\n",
            "295/295 [==============================] - 0s 890us/sample - loss: 0.3824 - acc: 0.8610 - val_loss: 0.8684 - val_acc: 0.6413\n",
            "Epoch 11/40\n",
            "295/295 [==============================] - 0s 831us/sample - loss: 0.2830 - acc: 0.8949 - val_loss: 1.0581 - val_acc: 0.5870\n",
            "Epoch 12/40\n",
            "295/295 [==============================] - 0s 811us/sample - loss: 0.2044 - acc: 0.9288 - val_loss: 1.3809 - val_acc: 0.6739\n",
            "Epoch 13/40\n",
            "295/295 [==============================] - 0s 817us/sample - loss: 0.1531 - acc: 0.9424 - val_loss: 1.4200 - val_acc: 0.6304\n",
            "Epoch 14/40\n",
            "295/295 [==============================] - 0s 825us/sample - loss: 0.1070 - acc: 0.9661 - val_loss: 1.7607 - val_acc: 0.5435\n",
            "Epoch 15/40\n",
            "295/295 [==============================] - 0s 823us/sample - loss: 0.0852 - acc: 0.9729 - val_loss: 2.0034 - val_acc: 0.6630\n",
            "Epoch 16/40\n",
            "295/295 [==============================] - 0s 829us/sample - loss: 0.0900 - acc: 0.9627 - val_loss: 2.1277 - val_acc: 0.5109\n",
            "Epoch 17/40\n",
            "295/295 [==============================] - 0s 823us/sample - loss: 0.0812 - acc: 0.9729 - val_loss: 2.3111 - val_acc: 0.6087\n",
            "Epoch 18/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 0.0798 - acc: 0.9797 - val_loss: 2.3647 - val_acc: 0.6196\n",
            "Epoch 19/40\n",
            "295/295 [==============================] - 0s 835us/sample - loss: 0.0535 - acc: 0.9831 - val_loss: 2.4301 - val_acc: 0.5978\n",
            "Epoch 20/40\n",
            "295/295 [==============================] - 0s 812us/sample - loss: 0.0131 - acc: 1.0000 - val_loss: 2.5812 - val_acc: 0.6739\n",
            "Epoch 21/40\n",
            "295/295 [==============================] - 0s 813us/sample - loss: 0.0054 - acc: 1.0000 - val_loss: 2.6633 - val_acc: 0.6848\n",
            "Epoch 22/40\n",
            "295/295 [==============================] - 0s 813us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 2.6915 - val_acc: 0.6848\n",
            "Epoch 23/40\n",
            "295/295 [==============================] - 0s 872us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 2.7072 - val_acc: 0.6630\n",
            "Epoch 24/40\n",
            "295/295 [==============================] - 0s 812us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 2.7507 - val_acc: 0.6630\n",
            "Epoch 25/40\n",
            "295/295 [==============================] - 0s 817us/sample - loss: 8.7279e-04 - acc: 1.0000 - val_loss: 2.7936 - val_acc: 0.6739\n",
            "Epoch 26/40\n",
            "295/295 [==============================] - 0s 829us/sample - loss: 7.7590e-04 - acc: 1.0000 - val_loss: 2.8240 - val_acc: 0.6630\n",
            "Epoch 27/40\n",
            "295/295 [==============================] - 0s 855us/sample - loss: 6.8557e-04 - acc: 1.0000 - val_loss: 2.8586 - val_acc: 0.6848\n",
            "Epoch 28/40\n",
            "295/295 [==============================] - 0s 825us/sample - loss: 6.1504e-04 - acc: 1.0000 - val_loss: 2.8868 - val_acc: 0.6848\n",
            "Epoch 29/40\n",
            "295/295 [==============================] - 0s 813us/sample - loss: 5.5594e-04 - acc: 1.0000 - val_loss: 2.9163 - val_acc: 0.6848\n",
            "Epoch 30/40\n",
            "295/295 [==============================] - 0s 816us/sample - loss: 5.0802e-04 - acc: 1.0000 - val_loss: 2.9386 - val_acc: 0.6848\n",
            "Epoch 31/40\n",
            "295/295 [==============================] - 0s 877us/sample - loss: 4.6462e-04 - acc: 1.0000 - val_loss: 2.9607 - val_acc: 0.6848\n",
            "Epoch 32/40\n",
            "295/295 [==============================] - 0s 861us/sample - loss: 4.3028e-04 - acc: 1.0000 - val_loss: 2.9851 - val_acc: 0.6848\n",
            "Epoch 33/40\n",
            "295/295 [==============================] - 0s 838us/sample - loss: 3.9784e-04 - acc: 1.0000 - val_loss: 3.0104 - val_acc: 0.6848\n",
            "Epoch 34/40\n",
            "295/295 [==============================] - 0s 841us/sample - loss: 3.6970e-04 - acc: 1.0000 - val_loss: 3.0342 - val_acc: 0.6848\n",
            "Epoch 35/40\n",
            "295/295 [==============================] - 0s 875us/sample - loss: 3.4373e-04 - acc: 1.0000 - val_loss: 3.0563 - val_acc: 0.6848\n",
            "Epoch 36/40\n",
            "295/295 [==============================] - 0s 832us/sample - loss: 3.2479e-04 - acc: 1.0000 - val_loss: 3.0731 - val_acc: 0.6848\n",
            "Epoch 37/40\n",
            "295/295 [==============================] - 0s 816us/sample - loss: 3.0345e-04 - acc: 1.0000 - val_loss: 3.0930 - val_acc: 0.6848\n",
            "Epoch 38/40\n",
            "295/295 [==============================] - 0s 814us/sample - loss: 2.8438e-04 - acc: 1.0000 - val_loss: 3.1134 - val_acc: 0.6848\n",
            "Epoch 39/40\n",
            "295/295 [==============================] - 0s 851us/sample - loss: 2.6751e-04 - acc: 1.0000 - val_loss: 3.1318 - val_acc: 0.6848\n",
            "Epoch 40/40\n",
            "295/295 [==============================] - 0s 823us/sample - loss: 2.5139e-04 - acc: 1.0000 - val_loss: 3.1500 - val_acc: 0.6848\n",
            "74/74 [==============================] - 0s 380us/sample - loss: 2.2043 - acc: 0.7027\n",
            "Train on 295 samples, validate on 92 samples\n",
            "Epoch 1/40\n",
            "295/295 [==============================] - 9s 29ms/sample - loss: 0.9087 - acc: 0.6746 - val_loss: 0.8828 - val_acc: 0.6957\n",
            "Epoch 2/40\n",
            "295/295 [==============================] - 0s 1ms/sample - loss: 0.8461 - acc: 0.6508 - val_loss: 0.8223 - val_acc: 0.6957\n",
            "Epoch 3/40\n",
            "295/295 [==============================] - 0s 985us/sample - loss: 0.8037 - acc: 0.6949 - val_loss: 0.8193 - val_acc: 0.6957\n",
            "Epoch 4/40\n",
            "295/295 [==============================] - 0s 925us/sample - loss: 0.7939 - acc: 0.6949 - val_loss: 0.7791 - val_acc: 0.6957\n",
            "Epoch 5/40\n",
            "295/295 [==============================] - 0s 967us/sample - loss: 0.7753 - acc: 0.6983 - val_loss: 0.7365 - val_acc: 0.7065\n",
            "Epoch 6/40\n",
            "295/295 [==============================] - 0s 867us/sample - loss: 0.7487 - acc: 0.7186 - val_loss: 0.7678 - val_acc: 0.7174\n",
            "Epoch 7/40\n",
            "295/295 [==============================] - 0s 849us/sample - loss: 0.7087 - acc: 0.7356 - val_loss: 0.8165 - val_acc: 0.6848\n",
            "Epoch 8/40\n",
            "295/295 [==============================] - 0s 885us/sample - loss: 0.6185 - acc: 0.7729 - val_loss: 0.7024 - val_acc: 0.7391\n",
            "Epoch 9/40\n",
            "295/295 [==============================] - 0s 888us/sample - loss: 0.4907 - acc: 0.8068 - val_loss: 0.8281 - val_acc: 0.7391\n",
            "Epoch 10/40\n",
            "295/295 [==============================] - 0s 834us/sample - loss: 0.4297 - acc: 0.8305 - val_loss: 0.8514 - val_acc: 0.6739\n",
            "Epoch 11/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 0.3191 - acc: 0.8780 - val_loss: 0.9395 - val_acc: 0.6522\n",
            "Epoch 12/40\n",
            "295/295 [==============================] - 0s 839us/sample - loss: 0.2214 - acc: 0.9254 - val_loss: 1.0717 - val_acc: 0.6630\n",
            "Epoch 13/40\n",
            "295/295 [==============================] - 0s 875us/sample - loss: 0.2366 - acc: 0.9186 - val_loss: 1.3066 - val_acc: 0.6739\n",
            "Epoch 14/40\n",
            "295/295 [==============================] - 0s 818us/sample - loss: 0.1453 - acc: 0.9525 - val_loss: 1.2072 - val_acc: 0.7174\n",
            "Epoch 15/40\n",
            "295/295 [==============================] - 0s 837us/sample - loss: 0.0573 - acc: 0.9898 - val_loss: 1.4762 - val_acc: 0.6957\n",
            "Epoch 16/40\n",
            "295/295 [==============================] - 0s 853us/sample - loss: 0.0210 - acc: 1.0000 - val_loss: 1.5937 - val_acc: 0.6739\n",
            "Epoch 17/40\n",
            "295/295 [==============================] - 0s 912us/sample - loss: 0.0112 - acc: 1.0000 - val_loss: 1.8216 - val_acc: 0.7065\n",
            "Epoch 18/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 1.9050 - val_acc: 0.6957\n",
            "Epoch 19/40\n",
            "295/295 [==============================] - 0s 838us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 1.9641 - val_acc: 0.7065\n",
            "Epoch 20/40\n",
            "295/295 [==============================] - 0s 829us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 2.0057 - val_acc: 0.7065\n",
            "Epoch 21/40\n",
            "295/295 [==============================] - 0s 865us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 2.0533 - val_acc: 0.6957\n",
            "Epoch 22/40\n",
            "295/295 [==============================] - 0s 855us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 2.0905 - val_acc: 0.7065\n",
            "Epoch 23/40\n",
            "295/295 [==============================] - 0s 848us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.1074 - val_acc: 0.7065\n",
            "Epoch 24/40\n",
            "295/295 [==============================] - 0s 824us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.1469 - val_acc: 0.7065\n",
            "Epoch 25/40\n",
            "295/295 [==============================] - 0s 849us/sample - loss: 9.3536e-04 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.7065\n",
            "Epoch 26/40\n",
            "295/295 [==============================] - 0s 832us/sample - loss: 8.5029e-04 - acc: 1.0000 - val_loss: 2.1960 - val_acc: 0.7065\n",
            "Epoch 27/40\n",
            "295/295 [==============================] - 0s 849us/sample - loss: 7.5494e-04 - acc: 1.0000 - val_loss: 2.2249 - val_acc: 0.7065\n",
            "Epoch 28/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 6.8481e-04 - acc: 1.0000 - val_loss: 2.2531 - val_acc: 0.7065\n",
            "Epoch 29/40\n",
            "295/295 [==============================] - 0s 858us/sample - loss: 6.2364e-04 - acc: 1.0000 - val_loss: 2.2691 - val_acc: 0.7065\n",
            "Epoch 30/40\n",
            "295/295 [==============================] - 0s 810us/sample - loss: 5.7098e-04 - acc: 1.0000 - val_loss: 2.2874 - val_acc: 0.6957\n",
            "Epoch 31/40\n",
            "295/295 [==============================] - 0s 841us/sample - loss: 5.2427e-04 - acc: 1.0000 - val_loss: 2.3080 - val_acc: 0.7065\n",
            "Epoch 32/40\n",
            "295/295 [==============================] - 0s 820us/sample - loss: 4.8448e-04 - acc: 1.0000 - val_loss: 2.3239 - val_acc: 0.7065\n",
            "Epoch 33/40\n",
            "295/295 [==============================] - 0s 876us/sample - loss: 4.5748e-04 - acc: 1.0000 - val_loss: 2.3595 - val_acc: 0.7065\n",
            "Epoch 34/40\n",
            "295/295 [==============================] - 0s 841us/sample - loss: 4.2744e-04 - acc: 1.0000 - val_loss: 2.3529 - val_acc: 0.6957\n",
            "Epoch 35/40\n",
            "295/295 [==============================] - 0s 839us/sample - loss: 3.8639e-04 - acc: 1.0000 - val_loss: 2.3814 - val_acc: 0.7065\n",
            "Epoch 36/40\n",
            "295/295 [==============================] - 0s 826us/sample - loss: 3.6207e-04 - acc: 1.0000 - val_loss: 2.3908 - val_acc: 0.6957\n",
            "Epoch 37/40\n",
            "295/295 [==============================] - 0s 836us/sample - loss: 3.4079e-04 - acc: 1.0000 - val_loss: 2.4156 - val_acc: 0.6957\n",
            "Epoch 38/40\n",
            "295/295 [==============================] - 0s 821us/sample - loss: 3.1772e-04 - acc: 1.0000 - val_loss: 2.4256 - val_acc: 0.6957\n",
            "Epoch 39/40\n",
            "295/295 [==============================] - 0s 827us/sample - loss: 2.9813e-04 - acc: 1.0000 - val_loss: 2.4364 - val_acc: 0.6957\n",
            "Epoch 40/40\n",
            "295/295 [==============================] - 0s 845us/sample - loss: 2.8446e-04 - acc: 1.0000 - val_loss: 2.4634 - val_acc: 0.7065\n",
            "74/74 [==============================] - 0s 383us/sample - loss: 2.5693 - acc: 0.7703\n",
            "Train on 295 samples, validate on 92 samples\n",
            "Epoch 1/40\n",
            "295/295 [==============================] - 9s 29ms/sample - loss: 0.8907 - acc: 0.6610 - val_loss: 0.8022 - val_acc: 0.6957\n",
            "Epoch 2/40\n",
            "295/295 [==============================] - 0s 1ms/sample - loss: 0.8403 - acc: 0.6949 - val_loss: 0.8257 - val_acc: 0.6957\n",
            "Epoch 3/40\n",
            "295/295 [==============================] - 0s 925us/sample - loss: 0.8297 - acc: 0.6949 - val_loss: 0.9918 - val_acc: 0.2935\n",
            "Epoch 4/40\n",
            "295/295 [==============================] - 0s 933us/sample - loss: 0.8322 - acc: 0.6508 - val_loss: 0.7857 - val_acc: 0.6957\n",
            "Epoch 5/40\n",
            "295/295 [==============================] - 0s 904us/sample - loss: 0.7761 - acc: 0.6949 - val_loss: 0.7495 - val_acc: 0.6957\n",
            "Epoch 6/40\n",
            "295/295 [==============================] - 0s 907us/sample - loss: 0.7149 - acc: 0.7322 - val_loss: 0.7213 - val_acc: 0.7174\n",
            "Epoch 7/40\n",
            "295/295 [==============================] - 0s 861us/sample - loss: 0.6600 - acc: 0.7390 - val_loss: 0.7112 - val_acc: 0.7174\n",
            "Epoch 8/40\n",
            "295/295 [==============================] - 0s 860us/sample - loss: 0.6124 - acc: 0.7627 - val_loss: 0.7469 - val_acc: 0.7391\n",
            "Epoch 9/40\n",
            "295/295 [==============================] - 0s 842us/sample - loss: 0.4923 - acc: 0.8203 - val_loss: 0.7931 - val_acc: 0.7391\n",
            "Epoch 10/40\n",
            "295/295 [==============================] - 0s 863us/sample - loss: 0.4184 - acc: 0.8542 - val_loss: 0.9310 - val_acc: 0.7391\n",
            "Epoch 11/40\n",
            "295/295 [==============================] - 0s 824us/sample - loss: 0.3038 - acc: 0.8780 - val_loss: 1.1393 - val_acc: 0.5978\n",
            "Epoch 12/40\n",
            "295/295 [==============================] - 0s 863us/sample - loss: 0.2453 - acc: 0.9186 - val_loss: 1.1876 - val_acc: 0.6957\n",
            "Epoch 13/40\n",
            "295/295 [==============================] - 0s 832us/sample - loss: 0.1777 - acc: 0.9424 - val_loss: 1.3326 - val_acc: 0.6957\n",
            "Epoch 14/40\n",
            "295/295 [==============================] - 0s 829us/sample - loss: 0.1210 - acc: 0.9525 - val_loss: 1.3356 - val_acc: 0.6739\n",
            "Epoch 15/40\n",
            "295/295 [==============================] - 0s 832us/sample - loss: 0.0627 - acc: 0.9864 - val_loss: 1.4026 - val_acc: 0.6848\n",
            "Epoch 16/40\n",
            "295/295 [==============================] - 0s 857us/sample - loss: 0.0228 - acc: 0.9966 - val_loss: 1.5699 - val_acc: 0.7283\n",
            "Epoch 17/40\n",
            "295/295 [==============================] - 0s 825us/sample - loss: 0.0115 - acc: 1.0000 - val_loss: 1.7108 - val_acc: 0.7174\n",
            "Epoch 18/40\n",
            "295/295 [==============================] - 0s 813us/sample - loss: 0.0050 - acc: 1.0000 - val_loss: 1.7940 - val_acc: 0.7065\n",
            "Epoch 19/40\n",
            "295/295 [==============================] - 0s 823us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 1.8975 - val_acc: 0.7283\n",
            "Epoch 20/40\n",
            "295/295 [==============================] - 0s 844us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 1.9757 - val_acc: 0.7174\n",
            "Epoch 21/40\n",
            "295/295 [==============================] - 0s 819us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 2.0287 - val_acc: 0.7174\n",
            "Epoch 22/40\n",
            "295/295 [==============================] - 0s 804us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 2.0775 - val_acc: 0.6957\n",
            "Epoch 23/40\n",
            "295/295 [==============================] - 0s 812us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.1195 - val_acc: 0.7065\n",
            "Epoch 24/40\n",
            "295/295 [==============================] - 0s 860us/sample - loss: 9.6417e-04 - acc: 1.0000 - val_loss: 2.1580 - val_acc: 0.7065\n",
            "Epoch 25/40\n",
            "295/295 [==============================] - 0s 819us/sample - loss: 8.4770e-04 - acc: 1.0000 - val_loss: 2.1947 - val_acc: 0.7065\n",
            "Epoch 26/40\n",
            "295/295 [==============================] - 0s 820us/sample - loss: 7.5448e-04 - acc: 1.0000 - val_loss: 2.2283 - val_acc: 0.7065\n",
            "Epoch 27/40\n",
            "295/295 [==============================] - 0s 816us/sample - loss: 6.7835e-04 - acc: 1.0000 - val_loss: 2.2576 - val_acc: 0.7065\n",
            "Epoch 28/40\n",
            "295/295 [==============================] - 0s 871us/sample - loss: 6.1400e-04 - acc: 1.0000 - val_loss: 2.2830 - val_acc: 0.7065\n",
            "Epoch 29/40\n",
            "295/295 [==============================] - 0s 839us/sample - loss: 5.5996e-04 - acc: 1.0000 - val_loss: 2.3080 - val_acc: 0.7065\n",
            "Epoch 30/40\n",
            "295/295 [==============================] - 0s 814us/sample - loss: 5.1281e-04 - acc: 1.0000 - val_loss: 2.3321 - val_acc: 0.7065\n",
            "Epoch 31/40\n",
            "295/295 [==============================] - 0s 815us/sample - loss: 4.8033e-04 - acc: 1.0000 - val_loss: 2.3577 - val_acc: 0.7174\n",
            "Epoch 32/40\n",
            "295/295 [==============================] - 0s 847us/sample - loss: 4.3479e-04 - acc: 1.0000 - val_loss: 2.3763 - val_acc: 0.7065\n",
            "Epoch 33/40\n",
            "295/295 [==============================] - 0s 815us/sample - loss: 4.0530e-04 - acc: 1.0000 - val_loss: 2.3993 - val_acc: 0.7174\n",
            "Epoch 34/40\n",
            "295/295 [==============================] - 0s 810us/sample - loss: 3.7503e-04 - acc: 1.0000 - val_loss: 2.4248 - val_acc: 0.7174\n",
            "Epoch 35/40\n",
            "295/295 [==============================] - 0s 824us/sample - loss: 3.5152e-04 - acc: 1.0000 - val_loss: 2.4449 - val_acc: 0.7174\n",
            "Epoch 36/40\n",
            "295/295 [==============================] - 0s 864us/sample - loss: 3.2871e-04 - acc: 1.0000 - val_loss: 2.4638 - val_acc: 0.7174\n",
            "Epoch 37/40\n",
            "295/295 [==============================] - 0s 829us/sample - loss: 3.0714e-04 - acc: 1.0000 - val_loss: 2.4817 - val_acc: 0.7174\n",
            "Epoch 38/40\n",
            "295/295 [==============================] - 0s 808us/sample - loss: 2.8890e-04 - acc: 1.0000 - val_loss: 2.4981 - val_acc: 0.7174\n",
            "Epoch 39/40\n",
            "295/295 [==============================] - 0s 817us/sample - loss: 2.7360e-04 - acc: 1.0000 - val_loss: 2.5141 - val_acc: 0.7174\n",
            "Epoch 40/40\n",
            "295/295 [==============================] - 0s 850us/sample - loss: 2.5674e-04 - acc: 1.0000 - val_loss: 2.5330 - val_acc: 0.7174\n",
            "74/74 [==============================] - 0s 386us/sample - loss: 1.7828 - acc: 0.6892\n",
            "Train on 295 samples, validate on 92 samples\n",
            "Epoch 1/40\n",
            "295/295 [==============================] - 9s 30ms/sample - loss: 0.9119 - acc: 0.6915 - val_loss: 0.8065 - val_acc: 0.6957\n",
            "Epoch 2/40\n",
            "295/295 [==============================] - 0s 1ms/sample - loss: 0.8429 - acc: 0.6949 - val_loss: 0.8346 - val_acc: 0.6957\n",
            "Epoch 3/40\n",
            "295/295 [==============================] - 0s 1ms/sample - loss: 0.8162 - acc: 0.6949 - val_loss: 0.7816 - val_acc: 0.6957\n",
            "Epoch 4/40\n",
            "295/295 [==============================] - 0s 908us/sample - loss: 0.8091 - acc: 0.7017 - val_loss: 0.8864 - val_acc: 0.6957\n",
            "Epoch 5/40\n",
            "295/295 [==============================] - 0s 948us/sample - loss: 0.7992 - acc: 0.7017 - val_loss: 0.7711 - val_acc: 0.6957\n",
            "Epoch 6/40\n",
            "295/295 [==============================] - 0s 897us/sample - loss: 0.7714 - acc: 0.7220 - val_loss: 0.8330 - val_acc: 0.7500\n",
            "Epoch 7/40\n",
            "295/295 [==============================] - 0s 859us/sample - loss: 0.7600 - acc: 0.7424 - val_loss: 0.6962 - val_acc: 0.7500\n",
            "Epoch 8/40\n",
            "295/295 [==============================] - 0s 914us/sample - loss: 0.6469 - acc: 0.7593 - val_loss: 0.6417 - val_acc: 0.7500\n",
            "Epoch 9/40\n",
            "295/295 [==============================] - 0s 921us/sample - loss: 0.5843 - acc: 0.7763 - val_loss: 0.6782 - val_acc: 0.7174\n",
            "Epoch 10/40\n",
            "295/295 [==============================] - 0s 892us/sample - loss: 0.5361 - acc: 0.8169 - val_loss: 0.6295 - val_acc: 0.7717\n",
            "Epoch 11/40\n",
            "295/295 [==============================] - 0s 857us/sample - loss: 0.4687 - acc: 0.8407 - val_loss: 0.7893 - val_acc: 0.7391\n",
            "Epoch 12/40\n",
            "295/295 [==============================] - 0s 854us/sample - loss: 0.3698 - acc: 0.8542 - val_loss: 0.7855 - val_acc: 0.7500\n",
            "Epoch 13/40\n",
            "295/295 [==============================] - 0s 853us/sample - loss: 0.2564 - acc: 0.9051 - val_loss: 0.9176 - val_acc: 0.7717\n",
            "Epoch 14/40\n",
            "295/295 [==============================] - 0s 864us/sample - loss: 0.1720 - acc: 0.9356 - val_loss: 0.9982 - val_acc: 0.6848\n",
            "Epoch 15/40\n",
            "295/295 [==============================] - 0s 835us/sample - loss: 0.1424 - acc: 0.9424 - val_loss: 1.1334 - val_acc: 0.7500\n",
            "Epoch 16/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 0.0720 - acc: 0.9763 - val_loss: 1.3898 - val_acc: 0.7609\n",
            "Epoch 17/40\n",
            "295/295 [==============================] - 0s 832us/sample - loss: 0.0374 - acc: 0.9898 - val_loss: 1.5984 - val_acc: 0.7391\n",
            "Epoch 18/40\n",
            "295/295 [==============================] - 0s 852us/sample - loss: 0.0147 - acc: 1.0000 - val_loss: 1.7089 - val_acc: 0.7174\n",
            "Epoch 19/40\n",
            "295/295 [==============================] - 0s 828us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 1.8908 - val_acc: 0.7391\n",
            "Epoch 20/40\n",
            "295/295 [==============================] - 0s 831us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 1.9315 - val_acc: 0.7500\n",
            "Epoch 21/40\n",
            "295/295 [==============================] - 0s 833us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 1.9993 - val_acc: 0.7500\n",
            "Epoch 22/40\n",
            "295/295 [==============================] - 0s 858us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 2.0689 - val_acc: 0.7500\n",
            "Epoch 23/40\n",
            "295/295 [==============================] - 0s 845us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 2.0938 - val_acc: 0.7500\n",
            "Epoch 24/40\n",
            "295/295 [==============================] - 0s 831us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.1342 - val_acc: 0.7500\n",
            "Epoch 25/40\n",
            "295/295 [==============================] - 0s 869us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 2.1663 - val_acc: 0.7500\n",
            "Epoch 26/40\n",
            "295/295 [==============================] - 0s 862us/sample - loss: 8.7990e-04 - acc: 1.0000 - val_loss: 2.1988 - val_acc: 0.7500\n",
            "Epoch 27/40\n",
            "295/295 [==============================] - 0s 839us/sample - loss: 7.8262e-04 - acc: 1.0000 - val_loss: 2.2313 - val_acc: 0.7500\n",
            "Epoch 28/40\n",
            "295/295 [==============================] - 0s 847us/sample - loss: 6.9534e-04 - acc: 1.0000 - val_loss: 2.2642 - val_acc: 0.7500\n",
            "Epoch 29/40\n",
            "295/295 [==============================] - 0s 852us/sample - loss: 6.3022e-04 - acc: 1.0000 - val_loss: 2.2865 - val_acc: 0.7391\n",
            "Epoch 30/40\n",
            "295/295 [==============================] - 0s 895us/sample - loss: 5.6700e-04 - acc: 1.0000 - val_loss: 2.3120 - val_acc: 0.7391\n",
            "Epoch 31/40\n",
            "295/295 [==============================] - 0s 844us/sample - loss: 5.2023e-04 - acc: 1.0000 - val_loss: 2.3362 - val_acc: 0.7391\n",
            "Epoch 32/40\n",
            "295/295 [==============================] - 0s 836us/sample - loss: 4.7296e-04 - acc: 1.0000 - val_loss: 2.3602 - val_acc: 0.7391\n",
            "Epoch 33/40\n",
            "295/295 [==============================] - 0s 835us/sample - loss: 4.3868e-04 - acc: 1.0000 - val_loss: 2.3818 - val_acc: 0.7391\n",
            "Epoch 34/40\n",
            "295/295 [==============================] - 0s 858us/sample - loss: 3.9759e-04 - acc: 1.0000 - val_loss: 2.3922 - val_acc: 0.7391\n",
            "Epoch 35/40\n",
            "295/295 [==============================] - 0s 834us/sample - loss: 3.7377e-04 - acc: 1.0000 - val_loss: 2.4198 - val_acc: 0.7391\n",
            "Epoch 36/40\n",
            "295/295 [==============================] - 0s 827us/sample - loss: 3.4449e-04 - acc: 1.0000 - val_loss: 2.4386 - val_acc: 0.7391\n",
            "Epoch 37/40\n",
            "295/295 [==============================] - 0s 828us/sample - loss: 3.2075e-04 - acc: 1.0000 - val_loss: 2.4567 - val_acc: 0.7391\n",
            "Epoch 38/40\n",
            "295/295 [==============================] - 0s 890us/sample - loss: 3.0066e-04 - acc: 1.0000 - val_loss: 2.4721 - val_acc: 0.7391\n",
            "Epoch 39/40\n",
            "295/295 [==============================] - 0s 822us/sample - loss: 2.8082e-04 - acc: 1.0000 - val_loss: 2.4924 - val_acc: 0.7391\n",
            "Epoch 40/40\n",
            "295/295 [==============================] - 0s 811us/sample - loss: 2.6360e-04 - acc: 1.0000 - val_loss: 2.5072 - val_acc: 0.7391\n",
            "74/74 [==============================] - 0s 362us/sample - loss: 2.9327 - acc: 0.6486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukhArOZ60a3",
        "colab_type": "code",
        "outputId": "421557e2-ca3b-4d7a-81b8-a5dbd0e170c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Calcul de la moyenne des précisions de chaque itération\n",
        "print(np.mean(cv))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7027027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 588
        }
      ]
    }
  ]
}