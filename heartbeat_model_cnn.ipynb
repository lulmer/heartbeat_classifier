{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "heartbeat_model_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lulmer/heartbeat_classifier/blob/master/heartbeat_model_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HAHudZEtns",
        "colab_type": "text"
      },
      "source": [
        "# Projet EDTS | Classification de battements de cœur\n",
        "---\n",
        "\n",
        "### Arnaud QUILLENT et Louis ULMER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb9wkBY9FJ_t",
        "colab_type": "text"
      },
      "source": [
        "# Architecture CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O7wI5a06nzy",
        "colab_type": "code",
        "outputId": "60c817a8-230d-44e1-8101-45b6160fbd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Montage du disque Google Drive sur le point /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzBEc6nz7ArN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chemin d'accès au dataset\n",
        "path = \"drive/My Drive/INSA/ASI 5/EDTS/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQBfDLc96lod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports des utilitaires\n",
        "import scipy.io.wavfile\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whHrFs3yUzTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(3581797089) # Fixation de la seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNALn7ZS6loh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = list(Path(path + \"dataset\").rglob(\"Btraining_*/*.wav\")) # Liste contenant le chemin d'accès aux fichiers du dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKD1KU4g6lo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction retournant le label d'un fichier\n",
        "def labelling(cats, file):\n",
        "    for i, cat in enumerate(cats):\n",
        "        if cat in file.parts[6]:\n",
        "            return i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sWYbSeJHbeT",
        "colab_type": "text"
      },
      "source": [
        "**Classes :**\n",
        "\n",
        "\n",
        "* `0` : Normal\n",
        "* `1` : Murmur\n",
        "* `2` : Extra-systole\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Xmuf7Z6lou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats = list(reversed(np.unique([file.parts[6] for file in files]))) # Noms des différentes classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CN_FnOTcRFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cats_files = [labelling(cats, file) for file in files] # Liste contenant tous les labels des fichiers du dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7q9agmM6lol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Séparation en jeux d'apprentissage, validation et test\n",
        "# Utilisation de stratify = mêmes proportions de classes dans chacun des jeux créés\n",
        "idx_train, idx_val = train_test_split(range(len(files)), test_size=0.3, stratify=cats_files)\n",
        "idx_train, idx_test = train_test_split(idx_train, test_size=33, stratify=np.array(cats_files)[idx_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALZS_HGc8Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Récupération des chemins vers les fichiers suivant la répartition en jeux d'apprentissage, validation et test\n",
        "files_train = np.array(files)[idx_train]\n",
        "files_val = np.array(files)[idx_val]\n",
        "files_test = np.array(files)[idx_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNyyNs938ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Récupération des labels des jeux d'apprentissage, validation et test\n",
        "y_train = np.array(cats_files)[idx_train]\n",
        "y_val = np.array(cats_files)[idx_val]\n",
        "y_test = np.array(cats_files)[idx_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj_uxEEp6lpJ",
        "colab_type": "code",
        "outputId": "19da3b8c-4d1c-4fcb-85d0-b9f3312e8f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "# Imports des modules de Deep Learning\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Dropout, Softmax, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, History, EarlyStopping\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n452TQCyZ86i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction convertissant une liste de fichiers en liste de spectrogrammes de Mel\n",
        "def to_spect(files):\n",
        "  x = np.empty((len(files), 128, 98)) # Initialisation de la matrice\n",
        "  for i, file in enumerate(tqdm(files)): # Pour chaque fichier de la liste (tqdm => affichage d'une barre de progression)\n",
        "    y, sr = librosa.load(file) # Ouverture du fichier, y = signal, sr = fréquence d'échantillonnage\n",
        "    # Pour que les spectrogrammes fassent tous la même taille, on fixe la longueur des signaux\n",
        "    if len(y) > 50000: # Si le signal est plus grand que 50000 valeurs\n",
        "      y = y[:50000] # On ne prend que les 50000 premières\n",
        "    elif len(y) < 50000: # Sinon\n",
        "      n_repet = int(np.ceil(50000/len(y))) # On calcule le nombre de répétitions du signal nécessaires\n",
        "      y = np.tile(y, n_repet)[:50000] # On répète le signal suivant ce nombre, et on en garde les 50000 premières valeurs\n",
        "    y = y/np.max(np.abs(y)) # Normalisation par le maximum\n",
        "    S = librosa.feature.melspectrogram(y, sr=sr, n_fft=2048, hop_length=512) # Calcul du spectrogramme de Mel\n",
        "    # (taille de fenêtre FFT = 2048, temps entre 2 frames = 512)(cf. papiers)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max) # Conversion en décibels\n",
        "    ss = (S_DB - np.mean(S_DB))/np.std(S_DB) # Réduction/centrage\n",
        "    ss = (ss - np.min(ss))/(np.max(ss)-np.min(ss)) # Normalisation min-max du spectrogramme => valeurs dans [0..1]\n",
        "    x[i,:,:] = ss # On ajoute le spectrogramme à la matrice\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41HOvbGjbHKC",
        "colab_type": "code",
        "outputId": "6e98386a-ed75-41af-b619-cc0e9a70db44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Création des matrices d'apprentissage, validation et test\n",
        "x_train = to_spect(files_train)\n",
        "x_val = to_spect(files_val)\n",
        "x_test = to_spect(files_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 289/289 [01:27<00:00,  3.16it/s]\n",
            "100%|██████████| 139/139 [00:43<00:00,  2.80it/s]\n",
            "100%|██████████| 33/33 [00:11<00:00,  3.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7JMSG-sgk5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN attendent une entrée de taille (TAILLE_BATCH, HAUTEUR, LARGEUR, CANAUX) => on ajoute une dimension de 1 pour nos canaux\n",
        "x_train = np.expand_dims(x_train, 3)\n",
        "x_val = np.expand_dims(x_val, 3)\n",
        "x_test = np.expand_dims(x_test, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsbKTIeJstSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modèle de CNN\n",
        "def CNN():\n",
        "  cnn = Sequential()\n",
        "  cnn.add(InputLayer(input_shape=(128, 98, 1))) # Tous les spectrogrammes sont de taille (128, 98)\n",
        "  cnn.add(Conv2D(32, 3)) # 32 filtres, fenêtre de taille (3,3)\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(64, 3)) # 64 filtres\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(128, 3)) # 128 filtres\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Conv2D(256, 3)) # 256 filtres\n",
        "  cnn.add(MaxPool2D(2))\n",
        "  cnn.add(Flatten()) # Conversion en un long vecteur de taille 256*hauteur*largeur\n",
        "  cnn.add(Dense(3)) # Full connection vers 3 classes\n",
        "  cnn.add(Softmax()) # Prédiction de probabilités\n",
        "  return cnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ath0uw2Usgnx",
        "colab_type": "code",
        "outputId": "56555d2f-a4fe-4252-8344-0979aae9e83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Affichage des couches, tailles et paramètres du modèle\n",
        "cnn = CNN()\n",
        "print(cnn.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 126, 96, 32)       320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 61, 46, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 21, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 8, 256)        295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 18435     \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 406,275\n",
            "Trainable params: 406,275\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJEQG4M6lpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilisation de l'optimiseur Adam, car propose un taux d'apprentissage adaptatif\n",
        "opti = Adam(lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9LtbnX6lpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compilation du graphe du modèle\n",
        "# Classification => Crossentropy\n",
        "# On cherche à maximiser la précision du modèle, en minimisant son entropie croisée\n",
        "cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=opti)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAernj9mIlrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callbacks Keras\n",
        "# Sauvegarde les poids du modèle à l'epoch où la précision est maximal\n",
        "chkpt_acc = ModelCheckpoint(filepath=path+\"best_acc.hdf5\", monitor=\"val_acc\", save_best_only=True, mode=\"max\", verbose=1)\n",
        "# Sauvegarde les poids du modèle à l'epoch où l'entropie est minimale\n",
        "chkpt_loss = ModelCheckpoint(filepath=path+\"best_loss.hdf5\", monitor=\"val_loss\", save_best_only=True, mode=\"min\", verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlOx9EZ56lpb",
        "colab_type": "code",
        "outputId": "037df842-ec9c-4e76-bbb9-facff95f6dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Apprentissage\n",
        "# Hyperparamètres choisis suite à plusieurs essais\n",
        "history = cnn.fit(x=x_train, y=to_categorical(y_train), epochs=40, batch_size=17, validation_data=(x_val, to_categorical(y_val)), callbacks=[chkpt_acc, chkpt_loss])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 83.5484 - acc: 0.4328\n",
            "Epoch 00001: val_acc improved from -inf to 0.62590, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.77649, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 3s 11ms/sample - loss: 85.6323 - acc: 0.4533 - val_loss: 8.7765 - val_acc: 0.6259\n",
            "Epoch 2/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 39.0695 - acc: 0.5176\n",
            "Epoch 00002: val_acc improved from 0.62590 to 0.69065, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 8.77649\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 36.3646 - acc: 0.5087 - val_loss: 17.3348 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 6.8621 - acc: 0.5490\n",
            "Epoch 00003: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00003: val_loss improved from 8.77649 to 2.89687, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 868us/sample - loss: 6.3828 - acc: 0.5121 - val_loss: 2.8969 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 2.8180 - acc: 0.4588\n",
            "Epoch 00004: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.89687 to 1.33625, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 831us/sample - loss: 2.7947 - acc: 0.4913 - val_loss: 1.3363 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "272/289 [===========================>..] - ETA: 0s - loss: 1.4203 - acc: 0.6140\n",
            "Epoch 00005: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.33625 to 1.27560, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 845us/sample - loss: 1.4546 - acc: 0.6090 - val_loss: 1.2756 - val_acc: 0.5755\n",
            "Epoch 6/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 1.0803 - acc: 0.6314\n",
            "Epoch 00006: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.27560 to 0.90181, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 895us/sample - loss: 1.0420 - acc: 0.6436 - val_loss: 0.9018 - val_acc: 0.6906\n",
            "Epoch 7/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.8318 - acc: 0.6933\n",
            "Epoch 00007: val_acc did not improve from 0.69065\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.90181 to 0.81889, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 893us/sample - loss: 0.8366 - acc: 0.6920 - val_loss: 0.8189 - val_acc: 0.6906\n",
            "Epoch 8/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.7769 - acc: 0.7059\n",
            "Epoch 00008: val_acc improved from 0.69065 to 0.69784, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.81889 to 0.77253, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7673 - acc: 0.7128 - val_loss: 0.7725 - val_acc: 0.6978\n",
            "Epoch 9/40\n",
            "272/289 [===========================>..] - ETA: 0s - loss: 0.7465 - acc: 0.7169\n",
            "Epoch 00009: val_acc improved from 0.69784 to 0.72662, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77253 to 0.75355, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7407 - acc: 0.7232 - val_loss: 0.7536 - val_acc: 0.7266\n",
            "Epoch 10/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.7284 - acc: 0.7294\n",
            "Epoch 00010: val_acc did not improve from 0.72662\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.75355\n",
            "289/289 [==============================] - 0s 609us/sample - loss: 0.7252 - acc: 0.7266 - val_loss: 0.8749 - val_acc: 0.5540\n",
            "Epoch 11/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.7635 - acc: 0.7017\n",
            "Epoch 00011: val_acc improved from 0.72662 to 0.74101, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.75355 to 0.73921, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.7428 - acc: 0.7197 - val_loss: 0.7392 - val_acc: 0.7410\n",
            "Epoch 12/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6804 - acc: 0.7451\n",
            "Epoch 00012: val_acc did not improve from 0.74101\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73921 to 0.73485, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 828us/sample - loss: 0.6891 - acc: 0.7439 - val_loss: 0.7349 - val_acc: 0.7410\n",
            "Epoch 13/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6762 - acc: 0.7686\n",
            "Epoch 00013: val_acc did not improve from 0.74101\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.73485\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 0.6827 - acc: 0.7578 - val_loss: 0.7875 - val_acc: 0.6547\n",
            "Epoch 14/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.7083 - acc: 0.7608\n",
            "Epoch 00014: val_acc did not improve from 0.74101\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.73485\n",
            "289/289 [==============================] - 0s 581us/sample - loss: 0.7072 - acc: 0.7578 - val_loss: 0.7382 - val_acc: 0.7338\n",
            "Epoch 15/40\n",
            "272/289 [===========================>..] - ETA: 0s - loss: 0.6802 - acc: 0.7463\n",
            "Epoch 00015: val_acc did not improve from 0.74101\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.73485 to 0.71929, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 834us/sample - loss: 0.6704 - acc: 0.7543 - val_loss: 0.7193 - val_acc: 0.7410\n",
            "Epoch 16/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6535 - acc: 0.7765\n",
            "Epoch 00016: val_acc improved from 0.74101 to 0.75540, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_acc.hdf5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71929 to 0.71306, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.6412 - acc: 0.7855 - val_loss: 0.7131 - val_acc: 0.7554\n",
            "Epoch 17/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6400 - acc: 0.7725\n",
            "Epoch 00017: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 0.6297 - acc: 0.7785 - val_loss: 0.7178 - val_acc: 0.7410\n",
            "Epoch 18/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6393 - acc: 0.7843\n",
            "Epoch 00018: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.6372 - acc: 0.7820 - val_loss: 0.7231 - val_acc: 0.6835\n",
            "Epoch 19/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6216 - acc: 0.7882\n",
            "Epoch 00019: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.6118 - acc: 0.7889 - val_loss: 0.7145 - val_acc: 0.7554\n",
            "Epoch 20/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.5867 - acc: 0.7941\n",
            "Epoch 00020: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 633us/sample - loss: 0.6024 - acc: 0.7855 - val_loss: 0.7194 - val_acc: 0.7554\n",
            "Epoch 21/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.5925 - acc: 0.7899\n",
            "Epoch 00021: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 649us/sample - loss: 0.5807 - acc: 0.7958 - val_loss: 0.7166 - val_acc: 0.7482\n",
            "Epoch 22/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5788 - acc: 0.7725\n",
            "Epoch 00022: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 602us/sample - loss: 0.5622 - acc: 0.7855 - val_loss: 0.7796 - val_acc: 0.7554\n",
            "Epoch 23/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6088 - acc: 0.7686\n",
            "Epoch 00023: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.71306\n",
            "289/289 [==============================] - 0s 578us/sample - loss: 0.5845 - acc: 0.7820 - val_loss: 0.7344 - val_acc: 0.7266\n",
            "Epoch 24/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5967 - acc: 0.7882\n",
            "Epoch 00024: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.71306 to 0.71242, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 804us/sample - loss: 0.6025 - acc: 0.7855 - val_loss: 0.7124 - val_acc: 0.7122\n",
            "Epoch 25/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.5207 - acc: 0.8193\n",
            "Epoch 00025: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.71242 to 0.70286, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 1ms/sample - loss: 0.5478 - acc: 0.8028 - val_loss: 0.7029 - val_acc: 0.7266\n",
            "Epoch 26/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5211 - acc: 0.8078\n",
            "Epoch 00026: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.70286 to 0.69134, saving model to drive/My Drive/INSA/ASI 5/EDTS/best_loss.hdf5\n",
            "289/289 [==============================] - 0s 790us/sample - loss: 0.5213 - acc: 0.8062 - val_loss: 0.6913 - val_acc: 0.7338\n",
            "Epoch 27/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5267 - acc: 0.8078\n",
            "Epoch 00027: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 593us/sample - loss: 0.5328 - acc: 0.8097 - val_loss: 0.7044 - val_acc: 0.7266\n",
            "Epoch 28/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5275 - acc: 0.7961\n",
            "Epoch 00028: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 610us/sample - loss: 0.5273 - acc: 0.7958 - val_loss: 0.7705 - val_acc: 0.6978\n",
            "Epoch 29/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5595 - acc: 0.7882\n",
            "Epoch 00029: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 578us/sample - loss: 0.5601 - acc: 0.7924 - val_loss: 0.7041 - val_acc: 0.7410\n",
            "Epoch 30/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.6433 - acc: 0.7451\n",
            "Epoch 00030: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 611us/sample - loss: 0.6331 - acc: 0.7509 - val_loss: 1.0122 - val_acc: 0.7194\n",
            "Epoch 31/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5556 - acc: 0.7922\n",
            "Epoch 00031: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 598us/sample - loss: 0.5433 - acc: 0.7993 - val_loss: 0.7801 - val_acc: 0.7482\n",
            "Epoch 32/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5302 - acc: 0.8000\n",
            "Epoch 00032: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 579us/sample - loss: 0.5473 - acc: 0.7855 - val_loss: 0.7740 - val_acc: 0.7410\n",
            "Epoch 33/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.5083 - acc: 0.8078\n",
            "Epoch 00033: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 618us/sample - loss: 0.5320 - acc: 0.8028 - val_loss: 0.7742 - val_acc: 0.7482\n",
            "Epoch 34/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.4963 - acc: 0.8235\n",
            "Epoch 00034: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 588us/sample - loss: 0.5028 - acc: 0.8201 - val_loss: 0.7490 - val_acc: 0.7554\n",
            "Epoch 35/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.4118 - acc: 0.8392\n",
            "Epoch 00035: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 587us/sample - loss: 0.4265 - acc: 0.8304 - val_loss: 0.7590 - val_acc: 0.7410\n",
            "Epoch 36/40\n",
            "238/289 [=======================>......] - ETA: 0s - loss: 0.4012 - acc: 0.8361\n",
            "Epoch 00036: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 625us/sample - loss: 0.4128 - acc: 0.8304 - val_loss: 0.7806 - val_acc: 0.6906\n",
            "Epoch 37/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.3864 - acc: 0.8549\n",
            "Epoch 00037: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 609us/sample - loss: 0.3913 - acc: 0.8512 - val_loss: 0.7728 - val_acc: 0.7410\n",
            "Epoch 38/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.3618 - acc: 0.8392\n",
            "Epoch 00038: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 0.3699 - acc: 0.8408 - val_loss: 0.8069 - val_acc: 0.7194\n",
            "Epoch 39/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.3681 - acc: 0.8667\n",
            "Epoch 00039: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.3693 - acc: 0.8685 - val_loss: 0.8121 - val_acc: 0.7050\n",
            "Epoch 40/40\n",
            "255/289 [=========================>....] - ETA: 0s - loss: 0.3284 - acc: 0.8784\n",
            "Epoch 00040: val_acc did not improve from 0.75540\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69134\n",
            "289/289 [==============================] - 0s 579us/sample - loss: 0.3213 - acc: 0.8789 - val_loss: 0.8051 - val_acc: 0.7338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvVbRfa26lpf",
        "colab_type": "code",
        "outputId": "f53744e9-9da2-440e-9e8a-790090143c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "# Affichage des courbes d'apprentissage\n",
        "plt.figure()\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.legend([\"Training\", \"Validation\"])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.figure()\n",
        "plt.plot(np.log(history.history['loss']))\n",
        "plt.plot(np.log(history.history['val_loss']))\n",
        "plt.legend([\"Training\", \"Validation\"])\n",
        "plt.title(\"LogLoss\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'LogLoss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncqWUCnD/H0pyO2VygtNTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o1Hnc_0-faj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14aIbSvnBZnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Chargement du modèle d'entropie minimale précédemment sauvegardé\n",
        "mdl = load_model(path + \"best_loss.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3lOp4OIBjbw",
        "colab_type": "code",
        "outputId": "b0a407ef-e830-4413-ff63-f05130c37030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Calcul du score de précision de ce modèle sur le jeu de test\n",
        "print(mdl.evaluate(x_test, to_categorical(y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 0s 2ms/sample - loss: 0.4795 - acc: 0.8485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.479492979519295, 0.8484849]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNkXKrBwGqIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = mdl.predict_classes(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5-pirp1Gm4c",
        "colab_type": "code",
        "outputId": "353ca524-87f6-496e-c6a9-715e65ead91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.96      0.90        23\n",
            "           1       0.86      0.86      0.86         7\n",
            "           2       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.85        33\n",
            "   macro avg       0.57      0.60      0.59        33\n",
            "weighted avg       0.77      0.85      0.81        33\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moc6tlRTFWxG",
        "colab_type": "text"
      },
      "source": [
        "# KFold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4A1AfRZBKBS",
        "colab": {}
      },
      "source": [
        "# StratifiedKFold => chaque sous ensemble possède la même répartition de classes que les autres => représentativité\n",
        "fold = StratifiedKFold(n_splits=5, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "16b479bb-5858-4829-e277-6270028590d5",
        "id": "P8SwbYE_BJic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calcul des spectrogrammes de tous les fichiers du dataset\n",
        "cv_all = to_spect(files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461/461 [03:19<00:00,  2.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljURAHw2BJC6",
        "colab": {}
      },
      "source": [
        "# Ajout d'une dimension pour les canaux\n",
        "cv_all = np.expand_dims(cv_all, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nce6D-29e1K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback\n",
        "# Stoppe l'apprentissage quand la fonction de coût n'évolue plus au bout de \"patience\" epochs\n",
        "# À la fin de l'apprentissage, les poids de l'epoch de coût minimal sons restaurés\n",
        "# => Permet de ne pas avoir à sauvegarder le meilleur modèle\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=40, mode=\"min\", restore_best_weights=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbWZBk2_5g1t",
        "colab_type": "code",
        "outputId": "04360f7d-4734-4946-8b24-8f4897dec62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Boucle de cross-validation\n",
        "cv = [] # Liste contenant les scores de précision de chaque itération\n",
        "for cv_train_idx, cv_val_idx in fold.split(cv_all, cats_files): # Récupération des indices de jeux d'apprentissage, validation et test de l'itération en cours\n",
        "  x_cv_val = cv_all[cv_val_idx] # Jeu de validation\n",
        "  y_cv_val = np.array(cats_files)[cv_val_idx] # Labels de validation\n",
        "  cv_tt_idx, cv_test_idx = train_test_split(cv_train_idx, test_size=33, stratify=np.array(cats_files)[cv_train_idx]) # Indices des jeux d'apprentissage et test\n",
        "  cv_x_train, cv_x_test = cv_all[cv_tt_idx], cv_all[cv_test_idx] # Jeux d'apprentissage et test\n",
        "  cv_y_train, cv_y_test = np.array(cats_files)[cv_tt_idx], np.array(cats_files)[cv_test_idx] # Labels d'apprentissage et test\n",
        "  model = CNN() # Création d'un nouveau modèle\n",
        "  model.compile(loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=\"adam\") # Compilation\n",
        "  model.fit(x_train, to_categorical(y_train), epochs=40, batch_size=17, validation_data=(x_val, to_categorical(y_val)), callbacks=[es]) # Apprentissage\n",
        "  cv.append(model.evaluate(x_test, to_categorical(y_test))[1]) # On ajoute la précision de l'itération à la liste"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 1.0568 - acc: 0.6436 - val_loss: 0.8423 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 0.8664 - acc: 0.6955 - val_loss: 0.8102 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "289/289 [==============================] - 0s 620us/sample - loss: 0.8214 - acc: 0.6955 - val_loss: 0.8269 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "289/289 [==============================] - 0s 618us/sample - loss: 0.7946 - acc: 0.6955 - val_loss: 0.7881 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "289/289 [==============================] - 0s 612us/sample - loss: 0.7866 - acc: 0.7059 - val_loss: 0.7737 - val_acc: 0.6978\n",
            "Epoch 6/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 0.7733 - acc: 0.7232 - val_loss: 0.7746 - val_acc: 0.7050\n",
            "Epoch 7/40\n",
            "289/289 [==============================] - 0s 638us/sample - loss: 0.7513 - acc: 0.7370 - val_loss: 0.7991 - val_acc: 0.6906\n",
            "Epoch 8/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.7252 - acc: 0.7336 - val_loss: 0.7320 - val_acc: 0.7266\n",
            "Epoch 9/40\n",
            "289/289 [==============================] - 0s 613us/sample - loss: 0.6477 - acc: 0.7612 - val_loss: 0.7039 - val_acc: 0.6978\n",
            "Epoch 10/40\n",
            "289/289 [==============================] - 0s 608us/sample - loss: 0.5704 - acc: 0.7855 - val_loss: 0.7011 - val_acc: 0.7266\n",
            "Epoch 11/40\n",
            "289/289 [==============================] - 0s 601us/sample - loss: 0.5180 - acc: 0.8201 - val_loss: 0.8141 - val_acc: 0.7338\n",
            "Epoch 12/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.4567 - acc: 0.8201 - val_loss: 0.7765 - val_acc: 0.7050\n",
            "Epoch 13/40\n",
            "289/289 [==============================] - 0s 642us/sample - loss: 0.3731 - acc: 0.8754 - val_loss: 0.8483 - val_acc: 0.7194\n",
            "Epoch 14/40\n",
            "289/289 [==============================] - 0s 619us/sample - loss: 0.2961 - acc: 0.8858 - val_loss: 0.9284 - val_acc: 0.6835\n",
            "Epoch 15/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.1983 - acc: 0.9446 - val_loss: 0.9982 - val_acc: 0.7050\n",
            "Epoch 16/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.1125 - acc: 0.9689 - val_loss: 1.2739 - val_acc: 0.6475\n",
            "Epoch 17/40\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 0.0931 - acc: 0.9689 - val_loss: 1.3714 - val_acc: 0.6763\n",
            "Epoch 18/40\n",
            "289/289 [==============================] - 0s 639us/sample - loss: 0.0441 - acc: 0.9931 - val_loss: 1.5379 - val_acc: 0.6906\n",
            "Epoch 19/40\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 0.0192 - acc: 1.0000 - val_loss: 1.7685 - val_acc: 0.6691\n",
            "Epoch 20/40\n",
            "289/289 [==============================] - 0s 603us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 1.9023 - val_acc: 0.6835\n",
            "Epoch 21/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 2.0463 - val_acc: 0.6978\n",
            "Epoch 22/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 2.0882 - val_acc: 0.6906\n",
            "Epoch 23/40\n",
            "289/289 [==============================] - 0s 620us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 2.1538 - val_acc: 0.6978\n",
            "Epoch 24/40\n",
            "289/289 [==============================] - 0s 641us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 2.1723 - val_acc: 0.6763\n",
            "Epoch 25/40\n",
            "289/289 [==============================] - 0s 617us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.2352 - val_acc: 0.6906\n",
            "Epoch 26/40\n",
            "289/289 [==============================] - 0s 582us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.2605 - val_acc: 0.6835\n",
            "Epoch 27/40\n",
            "289/289 [==============================] - 0s 592us/sample - loss: 9.2143e-04 - acc: 1.0000 - val_loss: 2.2734 - val_acc: 0.6835\n",
            "Epoch 28/40\n",
            "289/289 [==============================] - 0s 603us/sample - loss: 8.1076e-04 - acc: 1.0000 - val_loss: 2.3116 - val_acc: 0.6835\n",
            "Epoch 29/40\n",
            "289/289 [==============================] - 0s 607us/sample - loss: 7.3006e-04 - acc: 1.0000 - val_loss: 2.3439 - val_acc: 0.6835\n",
            "Epoch 30/40\n",
            "289/289 [==============================] - 0s 643us/sample - loss: 6.5906e-04 - acc: 1.0000 - val_loss: 2.3631 - val_acc: 0.6835\n",
            "Epoch 31/40\n",
            "289/289 [==============================] - 0s 617us/sample - loss: 5.9789e-04 - acc: 1.0000 - val_loss: 2.3880 - val_acc: 0.6835\n",
            "Epoch 32/40\n",
            "289/289 [==============================] - 0s 580us/sample - loss: 5.4016e-04 - acc: 1.0000 - val_loss: 2.4123 - val_acc: 0.6835\n",
            "Epoch 33/40\n",
            "289/289 [==============================] - 0s 555us/sample - loss: 4.9948e-04 - acc: 1.0000 - val_loss: 2.4330 - val_acc: 0.6835\n",
            "Epoch 34/40\n",
            "289/289 [==============================] - 0s 569us/sample - loss: 4.6150e-04 - acc: 1.0000 - val_loss: 2.4530 - val_acc: 0.6835\n",
            "Epoch 35/40\n",
            "289/289 [==============================] - 0s 618us/sample - loss: 4.2513e-04 - acc: 1.0000 - val_loss: 2.4694 - val_acc: 0.6835\n",
            "Epoch 36/40\n",
            "289/289 [==============================] - 0s 625us/sample - loss: 3.9703e-04 - acc: 1.0000 - val_loss: 2.4850 - val_acc: 0.6835\n",
            "Epoch 37/40\n",
            "289/289 [==============================] - 0s 591us/sample - loss: 3.6784e-04 - acc: 1.0000 - val_loss: 2.5125 - val_acc: 0.6835\n",
            "Epoch 38/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 3.4353e-04 - acc: 1.0000 - val_loss: 2.5286 - val_acc: 0.6835\n",
            "Epoch 39/40\n",
            "289/289 [==============================] - 0s 628us/sample - loss: 3.2620e-04 - acc: 1.0000 - val_loss: 2.5335 - val_acc: 0.6835\n",
            "Epoch 40/40\n",
            "289/289 [==============================] - 0s 593us/sample - loss: 3.0299e-04 - acc: 1.0000 - val_loss: 2.5640 - val_acc: 0.6835\n",
            "33/33 [==============================] - 0s 429us/sample - loss: 1.1551 - acc: 0.7879\n",
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.8998 - acc: 0.6955 - val_loss: 0.8136 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 0.8416 - acc: 0.6955 - val_loss: 0.8005 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "289/289 [==============================] - 0s 590us/sample - loss: 0.7940 - acc: 0.6955 - val_loss: 0.7938 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "289/289 [==============================] - 0s 601us/sample - loss: 0.8102 - acc: 0.6782 - val_loss: 0.8428 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "289/289 [==============================] - 0s 581us/sample - loss: 0.7854 - acc: 0.7163 - val_loss: 0.7680 - val_acc: 0.6906\n",
            "Epoch 6/40\n",
            "289/289 [==============================] - 0s 633us/sample - loss: 0.7249 - acc: 0.7301 - val_loss: 0.7343 - val_acc: 0.7194\n",
            "Epoch 7/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.6673 - acc: 0.7543 - val_loss: 0.7473 - val_acc: 0.7122\n",
            "Epoch 8/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.5681 - acc: 0.7958 - val_loss: 0.9259 - val_acc: 0.6475\n",
            "Epoch 9/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.4936 - acc: 0.8131 - val_loss: 0.9010 - val_acc: 0.7266\n",
            "Epoch 10/40\n",
            "289/289 [==============================] - 0s 619us/sample - loss: 0.4281 - acc: 0.8512 - val_loss: 0.7828 - val_acc: 0.7194\n",
            "Epoch 11/40\n",
            "289/289 [==============================] - 0s 603us/sample - loss: 0.3314 - acc: 0.8685 - val_loss: 0.8916 - val_acc: 0.6619\n",
            "Epoch 12/40\n",
            "289/289 [==============================] - 0s 607us/sample - loss: 0.2186 - acc: 0.9377 - val_loss: 1.0067 - val_acc: 0.6403\n",
            "Epoch 13/40\n",
            "289/289 [==============================] - 0s 605us/sample - loss: 0.1505 - acc: 0.9446 - val_loss: 1.3166 - val_acc: 0.6043\n",
            "Epoch 14/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.1317 - acc: 0.9654 - val_loss: 1.3330 - val_acc: 0.6691\n",
            "Epoch 15/40\n",
            "289/289 [==============================] - 0s 562us/sample - loss: 0.0983 - acc: 0.9689 - val_loss: 1.6226 - val_acc: 0.6763\n",
            "Epoch 16/40\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 0.0554 - acc: 0.9896 - val_loss: 1.7188 - val_acc: 0.6187\n",
            "Epoch 17/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.0342 - acc: 0.9896 - val_loss: 1.6878 - val_acc: 0.6763\n",
            "Epoch 18/40\n",
            "289/289 [==============================] - 0s 627us/sample - loss: 0.0143 - acc: 0.9965 - val_loss: 1.8263 - val_acc: 0.6835\n",
            "Epoch 19/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 2.0004 - val_acc: 0.6547\n",
            "Epoch 20/40\n",
            "289/289 [==============================] - 0s 571us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 2.0691 - val_acc: 0.6475\n",
            "Epoch 21/40\n",
            "289/289 [==============================] - 0s 555us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 2.1093 - val_acc: 0.6475\n",
            "Epoch 22/40\n",
            "289/289 [==============================] - 0s 562us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.1455 - val_acc: 0.6475\n",
            "Epoch 23/40\n",
            "289/289 [==============================] - 0s 559us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 2.1791 - val_acc: 0.6475\n",
            "Epoch 24/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 8.6402e-04 - acc: 1.0000 - val_loss: 2.2034 - val_acc: 0.6475\n",
            "Epoch 25/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 7.5591e-04 - acc: 1.0000 - val_loss: 2.2299 - val_acc: 0.6475\n",
            "Epoch 26/40\n",
            "289/289 [==============================] - 0s 613us/sample - loss: 6.6728e-04 - acc: 1.0000 - val_loss: 2.2537 - val_acc: 0.6475\n",
            "Epoch 27/40\n",
            "289/289 [==============================] - 0s 605us/sample - loss: 5.9892e-04 - acc: 1.0000 - val_loss: 2.2762 - val_acc: 0.6475\n",
            "Epoch 28/40\n",
            "289/289 [==============================] - 0s 617us/sample - loss: 5.4204e-04 - acc: 1.0000 - val_loss: 2.2972 - val_acc: 0.6403\n",
            "Epoch 29/40\n",
            "289/289 [==============================] - 0s 649us/sample - loss: 4.9381e-04 - acc: 1.0000 - val_loss: 2.3160 - val_acc: 0.6403\n",
            "Epoch 30/40\n",
            "289/289 [==============================] - 0s 626us/sample - loss: 4.5278e-04 - acc: 1.0000 - val_loss: 2.3335 - val_acc: 0.6403\n",
            "Epoch 31/40\n",
            "289/289 [==============================] - 0s 602us/sample - loss: 4.1766e-04 - acc: 1.0000 - val_loss: 2.3513 - val_acc: 0.6403\n",
            "Epoch 32/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 3.8391e-04 - acc: 1.0000 - val_loss: 2.3684 - val_acc: 0.6403\n",
            "Epoch 33/40\n",
            "289/289 [==============================] - 0s 585us/sample - loss: 3.5857e-04 - acc: 1.0000 - val_loss: 2.3838 - val_acc: 0.6403\n",
            "Epoch 34/40\n",
            "289/289 [==============================] - 0s 565us/sample - loss: 3.3400e-04 - acc: 1.0000 - val_loss: 2.3991 - val_acc: 0.6403\n",
            "Epoch 35/40\n",
            "289/289 [==============================] - 0s 602us/sample - loss: 3.1182e-04 - acc: 1.0000 - val_loss: 2.4143 - val_acc: 0.6475\n",
            "Epoch 36/40\n",
            "289/289 [==============================] - 0s 587us/sample - loss: 2.9318e-04 - acc: 1.0000 - val_loss: 2.4275 - val_acc: 0.6403\n",
            "Epoch 37/40\n",
            "289/289 [==============================] - 0s 591us/sample - loss: 2.7408e-04 - acc: 1.0000 - val_loss: 2.4422 - val_acc: 0.6475\n",
            "Epoch 38/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 2.5807e-04 - acc: 1.0000 - val_loss: 2.4560 - val_acc: 0.6403\n",
            "Epoch 39/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 2.4352e-04 - acc: 1.0000 - val_loss: 2.4696 - val_acc: 0.6403\n",
            "Epoch 40/40\n",
            "289/289 [==============================] - 0s 581us/sample - loss: 2.3001e-04 - acc: 1.0000 - val_loss: 2.4829 - val_acc: 0.6403\n",
            "33/33 [==============================] - 0s 501us/sample - loss: 1.4771 - acc: 0.7273\n",
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "289/289 [==============================] - 0s 2ms/sample - loss: 0.9620 - acc: 0.6228 - val_loss: 0.8999 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "289/289 [==============================] - 0s 590us/sample - loss: 0.8356 - acc: 0.6955 - val_loss: 0.8558 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "289/289 [==============================] - 0s 569us/sample - loss: 0.8349 - acc: 0.6990 - val_loss: 0.8831 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "289/289 [==============================] - 0s 593us/sample - loss: 0.8112 - acc: 0.6955 - val_loss: 0.7955 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "289/289 [==============================] - 0s 565us/sample - loss: 0.7829 - acc: 0.6955 - val_loss: 0.8052 - val_acc: 0.6906\n",
            "Epoch 6/40\n",
            "289/289 [==============================] - 0s 617us/sample - loss: 0.7782 - acc: 0.6990 - val_loss: 0.7793 - val_acc: 0.7122\n",
            "Epoch 7/40\n",
            "289/289 [==============================] - 0s 615us/sample - loss: 0.6988 - acc: 0.7751 - val_loss: 0.7244 - val_acc: 0.7050\n",
            "Epoch 8/40\n",
            "289/289 [==============================] - 0s 622us/sample - loss: 0.6453 - acc: 0.7716 - val_loss: 0.7362 - val_acc: 0.7194\n",
            "Epoch 9/40\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 0.5839 - acc: 0.7785 - val_loss: 0.7279 - val_acc: 0.7410\n",
            "Epoch 10/40\n",
            "289/289 [==============================] - 0s 576us/sample - loss: 0.4902 - acc: 0.8166 - val_loss: 0.7664 - val_acc: 0.7122\n",
            "Epoch 11/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 0.3742 - acc: 0.8685 - val_loss: 0.8504 - val_acc: 0.7050\n",
            "Epoch 12/40\n",
            "289/289 [==============================] - 0s 575us/sample - loss: 0.2964 - acc: 0.8824 - val_loss: 0.9841 - val_acc: 0.6906\n",
            "Epoch 13/40\n",
            "289/289 [==============================] - 0s 579us/sample - loss: 0.1902 - acc: 0.9308 - val_loss: 1.3083 - val_acc: 0.6187\n",
            "Epoch 14/40\n",
            "289/289 [==============================] - 0s 598us/sample - loss: 0.1314 - acc: 0.9689 - val_loss: 1.3207 - val_acc: 0.6978\n",
            "Epoch 15/40\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 0.0498 - acc: 0.9896 - val_loss: 1.4577 - val_acc: 0.6835\n",
            "Epoch 16/40\n",
            "289/289 [==============================] - 0s 595us/sample - loss: 0.0407 - acc: 0.9862 - val_loss: 1.8121 - val_acc: 0.6906\n",
            "Epoch 17/40\n",
            "289/289 [==============================] - 0s 619us/sample - loss: 0.0313 - acc: 0.9896 - val_loss: 2.0245 - val_acc: 0.6835\n",
            "Epoch 18/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.0192 - acc: 1.0000 - val_loss: 2.0610 - val_acc: 0.6906\n",
            "Epoch 19/40\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 2.1046 - val_acc: 0.6619\n",
            "Epoch 20/40\n",
            "289/289 [==============================] - 0s 575us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 2.1464 - val_acc: 0.6906\n",
            "Epoch 21/40\n",
            "289/289 [==============================] - 0s 587us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 2.1714 - val_acc: 0.6906\n",
            "Epoch 22/40\n",
            "289/289 [==============================] - 0s 631us/sample - loss: 9.4551e-04 - acc: 1.0000 - val_loss: 2.2000 - val_acc: 0.6835\n",
            "Epoch 23/40\n",
            "289/289 [==============================] - 0s 655us/sample - loss: 8.0697e-04 - acc: 1.0000 - val_loss: 2.2259 - val_acc: 0.6835\n",
            "Epoch 24/40\n",
            "289/289 [==============================] - 0s 624us/sample - loss: 7.0360e-04 - acc: 1.0000 - val_loss: 2.2544 - val_acc: 0.6835\n",
            "Epoch 25/40\n",
            "289/289 [==============================] - 0s 630us/sample - loss: 6.2363e-04 - acc: 1.0000 - val_loss: 2.2796 - val_acc: 0.6835\n",
            "Epoch 26/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 5.6684e-04 - acc: 1.0000 - val_loss: 2.3087 - val_acc: 0.6906\n",
            "Epoch 27/40\n",
            "289/289 [==============================] - 0s 577us/sample - loss: 5.0220e-04 - acc: 1.0000 - val_loss: 2.3243 - val_acc: 0.6835\n",
            "Epoch 28/40\n",
            "289/289 [==============================] - 0s 585us/sample - loss: 4.6391e-04 - acc: 1.0000 - val_loss: 2.3407 - val_acc: 0.6835\n",
            "Epoch 29/40\n",
            "289/289 [==============================] - 0s 590us/sample - loss: 4.2491e-04 - acc: 1.0000 - val_loss: 2.3686 - val_acc: 0.6906\n",
            "Epoch 30/40\n",
            "289/289 [==============================] - 0s 632us/sample - loss: 3.8821e-04 - acc: 1.0000 - val_loss: 2.3872 - val_acc: 0.6978\n",
            "Epoch 31/40\n",
            "289/289 [==============================] - 0s 589us/sample - loss: 3.5922e-04 - acc: 1.0000 - val_loss: 2.3952 - val_acc: 0.6906\n",
            "Epoch 32/40\n",
            "289/289 [==============================] - 0s 614us/sample - loss: 3.3349e-04 - acc: 1.0000 - val_loss: 2.4197 - val_acc: 0.6978\n",
            "Epoch 33/40\n",
            "289/289 [==============================] - 0s 627us/sample - loss: 3.0913e-04 - acc: 1.0000 - val_loss: 2.4342 - val_acc: 0.6978\n",
            "Epoch 34/40\n",
            "289/289 [==============================] - 0s 633us/sample - loss: 2.8988e-04 - acc: 1.0000 - val_loss: 2.4487 - val_acc: 0.6978\n",
            "Epoch 35/40\n",
            "289/289 [==============================] - 0s 610us/sample - loss: 2.7008e-04 - acc: 1.0000 - val_loss: 2.4669 - val_acc: 0.6978\n",
            "Epoch 36/40\n",
            "289/289 [==============================] - 0s 579us/sample - loss: 2.5341e-04 - acc: 1.0000 - val_loss: 2.4828 - val_acc: 0.6978\n",
            "Epoch 37/40\n",
            "289/289 [==============================] - 0s 588us/sample - loss: 2.3780e-04 - acc: 1.0000 - val_loss: 2.4945 - val_acc: 0.6978\n",
            "Epoch 38/40\n",
            "289/289 [==============================] - 0s 605us/sample - loss: 2.2514e-04 - acc: 1.0000 - val_loss: 2.5070 - val_acc: 0.6978\n",
            "Epoch 39/40\n",
            "289/289 [==============================] - 0s 600us/sample - loss: 2.1297e-04 - acc: 1.0000 - val_loss: 2.5229 - val_acc: 0.6978\n",
            "Epoch 40/40\n",
            "289/289 [==============================] - 0s 611us/sample - loss: 2.0110e-04 - acc: 1.0000 - val_loss: 2.5354 - val_acc: 0.6978\n",
            "33/33 [==============================] - 0s 305us/sample - loss: 1.4217 - acc: 0.7879\n",
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.9418 - acc: 0.6574 - val_loss: 0.8150 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.8344 - acc: 0.6782 - val_loss: 0.8377 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "289/289 [==============================] - 0s 632us/sample - loss: 0.8231 - acc: 0.6955 - val_loss: 0.8024 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "289/289 [==============================] - 0s 643us/sample - loss: 0.8061 - acc: 0.6955 - val_loss: 0.8263 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 0.8093 - acc: 0.6955 - val_loss: 0.8170 - val_acc: 0.6906\n",
            "Epoch 6/40\n",
            "289/289 [==============================] - 0s 589us/sample - loss: 0.7558 - acc: 0.7370 - val_loss: 0.7730 - val_acc: 0.6906\n",
            "Epoch 7/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.6973 - acc: 0.7266 - val_loss: 0.9215 - val_acc: 0.5755\n",
            "Epoch 8/40\n",
            "289/289 [==============================] - 0s 592us/sample - loss: 0.6614 - acc: 0.7474 - val_loss: 0.9199 - val_acc: 0.6475\n",
            "Epoch 9/40\n",
            "289/289 [==============================] - 0s 590us/sample - loss: 0.5797 - acc: 0.7889 - val_loss: 0.7145 - val_acc: 0.7338\n",
            "Epoch 10/40\n",
            "289/289 [==============================] - 0s 616us/sample - loss: 0.5580 - acc: 0.7993 - val_loss: 0.6819 - val_acc: 0.7194\n",
            "Epoch 11/40\n",
            "289/289 [==============================] - 0s 581us/sample - loss: 0.4393 - acc: 0.8339 - val_loss: 0.7407 - val_acc: 0.7482\n",
            "Epoch 12/40\n",
            "289/289 [==============================] - 0s 580us/sample - loss: 0.4474 - acc: 0.8304 - val_loss: 0.7848 - val_acc: 0.7266\n",
            "Epoch 13/40\n",
            "289/289 [==============================] - 0s 617us/sample - loss: 0.3497 - acc: 0.8789 - val_loss: 0.8434 - val_acc: 0.7266\n",
            "Epoch 14/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 0.2471 - acc: 0.9170 - val_loss: 1.0525 - val_acc: 0.7338\n",
            "Epoch 15/40\n",
            "289/289 [==============================] - 0s 570us/sample - loss: 0.2098 - acc: 0.9343 - val_loss: 1.0583 - val_acc: 0.6978\n",
            "Epoch 16/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.0943 - acc: 0.9792 - val_loss: 1.2257 - val_acc: 0.6835\n",
            "Epoch 17/40\n",
            "289/289 [==============================] - 0s 571us/sample - loss: 0.0353 - acc: 1.0000 - val_loss: 1.3992 - val_acc: 0.6978\n",
            "Epoch 18/40\n",
            "289/289 [==============================] - 0s 587us/sample - loss: 0.0116 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.7050\n",
            "Epoch 19/40\n",
            "289/289 [==============================] - 0s 600us/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 1.6580 - val_acc: 0.7050\n",
            "Epoch 20/40\n",
            "289/289 [==============================] - 0s 603us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 1.7417 - val_acc: 0.6906\n",
            "Epoch 21/40\n",
            "289/289 [==============================] - 0s 638us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 0.7050\n",
            "Epoch 22/40\n",
            "289/289 [==============================] - 0s 601us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 1.8572 - val_acc: 0.7050\n",
            "Epoch 23/40\n",
            "289/289 [==============================] - 0s 590us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 1.8979 - val_acc: 0.7050\n",
            "Epoch 24/40\n",
            "289/289 [==============================] - 0s 569us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.9364 - val_acc: 0.7050\n",
            "Epoch 25/40\n",
            "289/289 [==============================] - 0s 572us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.9759 - val_acc: 0.6978\n",
            "Epoch 26/40\n",
            "289/289 [==============================] - 0s 607us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.0142 - val_acc: 0.7050\n",
            "Epoch 27/40\n",
            "289/289 [==============================] - 0s 631us/sample - loss: 9.5848e-04 - acc: 1.0000 - val_loss: 2.0362 - val_acc: 0.6978\n",
            "Epoch 28/40\n",
            "289/289 [==============================] - 0s 612us/sample - loss: 8.6203e-04 - acc: 1.0000 - val_loss: 2.0705 - val_acc: 0.6978\n",
            "Epoch 29/40\n",
            "289/289 [==============================] - 0s 578us/sample - loss: 7.7329e-04 - acc: 1.0000 - val_loss: 2.0947 - val_acc: 0.6978\n",
            "Epoch 30/40\n",
            "289/289 [==============================] - 0s 589us/sample - loss: 6.9498e-04 - acc: 1.0000 - val_loss: 2.1237 - val_acc: 0.7050\n",
            "Epoch 31/40\n",
            "289/289 [==============================] - 0s 593us/sample - loss: 6.3373e-04 - acc: 1.0000 - val_loss: 2.1437 - val_acc: 0.6978\n",
            "Epoch 32/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 5.7708e-04 - acc: 1.0000 - val_loss: 2.1673 - val_acc: 0.6978\n",
            "Epoch 33/40\n",
            "289/289 [==============================] - 0s 597us/sample - loss: 5.2858e-04 - acc: 1.0000 - val_loss: 2.1926 - val_acc: 0.6978\n",
            "Epoch 34/40\n",
            "289/289 [==============================] - 0s 577us/sample - loss: 4.8922e-04 - acc: 1.0000 - val_loss: 2.2098 - val_acc: 0.6978\n",
            "Epoch 35/40\n",
            "289/289 [==============================] - 0s 608us/sample - loss: 4.5273e-04 - acc: 1.0000 - val_loss: 2.2305 - val_acc: 0.6978\n",
            "Epoch 36/40\n",
            "289/289 [==============================] - 0s 631us/sample - loss: 4.2100e-04 - acc: 1.0000 - val_loss: 2.2523 - val_acc: 0.6978\n",
            "Epoch 37/40\n",
            "289/289 [==============================] - 0s 614us/sample - loss: 3.9041e-04 - acc: 1.0000 - val_loss: 2.2666 - val_acc: 0.6978\n",
            "Epoch 38/40\n",
            "289/289 [==============================] - 0s 623us/sample - loss: 3.6621e-04 - acc: 1.0000 - val_loss: 2.2829 - val_acc: 0.6978\n",
            "Epoch 39/40\n",
            "289/289 [==============================] - 0s 601us/sample - loss: 3.4377e-04 - acc: 1.0000 - val_loss: 2.2987 - val_acc: 0.6978\n",
            "Epoch 40/40\n",
            "289/289 [==============================] - 0s 580us/sample - loss: 3.2144e-04 - acc: 1.0000 - val_loss: 2.3212 - val_acc: 0.6978\n",
            "33/33 [==============================] - 0s 307us/sample - loss: 1.3767 - acc: 0.7879\n",
            "Train on 289 samples, validate on 139 samples\n",
            "Epoch 1/40\n",
            "289/289 [==============================] - 1s 2ms/sample - loss: 0.8752 - acc: 0.6540 - val_loss: 0.8187 - val_acc: 0.6906\n",
            "Epoch 2/40\n",
            "289/289 [==============================] - 0s 589us/sample - loss: 0.8291 - acc: 0.6955 - val_loss: 0.8433 - val_acc: 0.6906\n",
            "Epoch 3/40\n",
            "289/289 [==============================] - 0s 615us/sample - loss: 0.8185 - acc: 0.6990 - val_loss: 0.7871 - val_acc: 0.6906\n",
            "Epoch 4/40\n",
            "289/289 [==============================] - 0s 598us/sample - loss: 0.7996 - acc: 0.6920 - val_loss: 0.7798 - val_acc: 0.6906\n",
            "Epoch 5/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 0.7614 - acc: 0.7163 - val_loss: 0.8122 - val_acc: 0.7122\n",
            "Epoch 6/40\n",
            "289/289 [==============================] - 0s 631us/sample - loss: 0.6767 - acc: 0.7612 - val_loss: 0.7225 - val_acc: 0.7410\n",
            "Epoch 7/40\n",
            "289/289 [==============================] - 0s 614us/sample - loss: 0.6118 - acc: 0.7993 - val_loss: 0.6712 - val_acc: 0.7410\n",
            "Epoch 8/40\n",
            "289/289 [==============================] - 0s 649us/sample - loss: 0.5081 - acc: 0.8270 - val_loss: 0.7523 - val_acc: 0.7338\n",
            "Epoch 9/40\n",
            "289/289 [==============================] - 0s 611us/sample - loss: 0.4162 - acc: 0.8478 - val_loss: 0.9598 - val_acc: 0.7194\n",
            "Epoch 10/40\n",
            "289/289 [==============================] - 0s 565us/sample - loss: 0.3195 - acc: 0.8927 - val_loss: 0.8575 - val_acc: 0.7410\n",
            "Epoch 11/40\n",
            "289/289 [==============================] - 0s 604us/sample - loss: 0.2211 - acc: 0.9239 - val_loss: 1.0401 - val_acc: 0.6978\n",
            "Epoch 12/40\n",
            "289/289 [==============================] - 0s 572us/sample - loss: 0.1316 - acc: 0.9481 - val_loss: 1.1753 - val_acc: 0.6835\n",
            "Epoch 13/40\n",
            "289/289 [==============================] - 0s 630us/sample - loss: 0.0506 - acc: 0.9965 - val_loss: 1.4312 - val_acc: 0.6906\n",
            "Epoch 14/40\n",
            "289/289 [==============================] - 0s 630us/sample - loss: 0.0158 - acc: 1.0000 - val_loss: 1.6558 - val_acc: 0.7050\n",
            "Epoch 15/40\n",
            "289/289 [==============================] - 0s 624us/sample - loss: 0.0067 - acc: 1.0000 - val_loss: 1.7894 - val_acc: 0.7122\n",
            "Epoch 16/40\n",
            "289/289 [==============================] - 0s 570us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 1.9190 - val_acc: 0.7050\n",
            "Epoch 17/40\n",
            "289/289 [==============================] - 0s 591us/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 1.9655 - val_acc: 0.7122\n",
            "Epoch 18/40\n",
            "289/289 [==============================] - 0s 606us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 2.0222 - val_acc: 0.7122\n",
            "Epoch 19/40\n",
            "289/289 [==============================] - 0s 579us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 2.0768 - val_acc: 0.7122\n",
            "Epoch 20/40\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.1168 - val_acc: 0.7050\n",
            "Epoch 21/40\n",
            "289/289 [==============================] - 0s 596us/sample - loss: 9.4090e-04 - acc: 1.0000 - val_loss: 2.1535 - val_acc: 0.7122\n",
            "Epoch 22/40\n",
            "289/289 [==============================] - 0s 615us/sample - loss: 8.2349e-04 - acc: 1.0000 - val_loss: 2.1944 - val_acc: 0.7122\n",
            "Epoch 23/40\n",
            "289/289 [==============================] - 0s 594us/sample - loss: 7.2122e-04 - acc: 1.0000 - val_loss: 2.2207 - val_acc: 0.7122\n",
            "Epoch 24/40\n",
            "289/289 [==============================] - 0s 616us/sample - loss: 6.4529e-04 - acc: 1.0000 - val_loss: 2.2580 - val_acc: 0.7122\n",
            "Epoch 25/40\n",
            "289/289 [==============================] - 0s 653us/sample - loss: 5.8789e-04 - acc: 1.0000 - val_loss: 2.2887 - val_acc: 0.7122\n",
            "Epoch 26/40\n",
            "289/289 [==============================] - 0s 584us/sample - loss: 5.1721e-04 - acc: 1.0000 - val_loss: 2.3104 - val_acc: 0.7122\n",
            "Epoch 27/40\n",
            "289/289 [==============================] - 0s 593us/sample - loss: 4.7160e-04 - acc: 1.0000 - val_loss: 2.3322 - val_acc: 0.7050\n",
            "Epoch 28/40\n",
            "289/289 [==============================] - 0s 574us/sample - loss: 4.3481e-04 - acc: 1.0000 - val_loss: 2.3629 - val_acc: 0.7122\n",
            "Epoch 29/40\n",
            "289/289 [==============================] - 0s 612us/sample - loss: 3.9245e-04 - acc: 1.0000 - val_loss: 2.3811 - val_acc: 0.7122\n",
            "Epoch 30/40\n",
            "289/289 [==============================] - 0s 599us/sample - loss: 3.6173e-04 - acc: 1.0000 - val_loss: 2.4048 - val_acc: 0.7122\n",
            "Epoch 31/40\n",
            "289/289 [==============================] - 0s 652us/sample - loss: 3.3413e-04 - acc: 1.0000 - val_loss: 2.4251 - val_acc: 0.7122\n",
            "Epoch 32/40\n",
            "289/289 [==============================] - 0s 598us/sample - loss: 3.1411e-04 - acc: 1.0000 - val_loss: 2.4516 - val_acc: 0.7122\n",
            "Epoch 33/40\n",
            "289/289 [==============================] - 0s 611us/sample - loss: 2.8888e-04 - acc: 1.0000 - val_loss: 2.4659 - val_acc: 0.7122\n",
            "Epoch 34/40\n",
            "289/289 [==============================] - 0s 585us/sample - loss: 2.6874e-04 - acc: 1.0000 - val_loss: 2.4844 - val_acc: 0.7122\n",
            "Epoch 35/40\n",
            "289/289 [==============================] - 0s 589us/sample - loss: 2.5112e-04 - acc: 1.0000 - val_loss: 2.5047 - val_acc: 0.7122\n",
            "Epoch 36/40\n",
            "289/289 [==============================] - 0s 583us/sample - loss: 2.3462e-04 - acc: 1.0000 - val_loss: 2.5196 - val_acc: 0.7122\n",
            "Epoch 37/40\n",
            "289/289 [==============================] - 0s 632us/sample - loss: 2.2135e-04 - acc: 1.0000 - val_loss: 2.5385 - val_acc: 0.7122\n",
            "Epoch 38/40\n",
            "289/289 [==============================] - 0s 586us/sample - loss: 2.0749e-04 - acc: 1.0000 - val_loss: 2.5500 - val_acc: 0.7122\n",
            "Epoch 39/40\n",
            "289/289 [==============================] - 0s 588us/sample - loss: 1.9615e-04 - acc: 1.0000 - val_loss: 2.5702 - val_acc: 0.7122\n",
            "Epoch 40/40\n",
            "289/289 [==============================] - 0s 583us/sample - loss: 1.8464e-04 - acc: 1.0000 - val_loss: 2.5834 - val_acc: 0.7050\n",
            "33/33 [==============================] - 0s 299us/sample - loss: 1.7450 - acc: 0.8182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukhArOZ60a3",
        "colab_type": "code",
        "outputId": "2b7b4c4b-e847-401e-9880-17de11535c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calcul de la moyenne des précisions de chaque itération\n",
        "print(\"Précision en cross-validation :\", np.mean(cv))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.78181815\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}